<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.8.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=7.4.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=7.4.1"><link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222"><link rel="alternate" href="/atom.xml" title="QIMING.INFO" type="application/atom+xml"><link rel="stylesheet" href="/css/main.css?v=7.4.1"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"always","offset":12,"onmobile":true,"dimmer":false},
    copycode: {"enable":true,"show_result":true},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"save":"auto"},
    fancybox: true,
    mediumzoom: ,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script><meta name="description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据"><meta name="keywords" content="Spark,HDFS,RDD,Hadoop,HBase"><meta property="og:type" content="article"><meta property="og:title" content="Spark从外部数据集中读取数据"><meta property="og:url" content="https://qiming.info/Spark从外部数据集中读取数据/index.html"><meta property="og:site_name" content="QIMING.INFO"><meta property="og:description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg"><meta property="og:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcryqj20g3040glj.jpg"><meta property="og:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszdh2kj20dm0deaah.jpg"><meta property="og:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszfc11j208p03rt8l.jpg"><meta property="og:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszgge9j208h03pwed.jpg"><meta property="og:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszck52j20kh02lq2w.jpg"><meta property="og:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszd8wyj20ka03y0su.jpg"><meta property="og:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqjlj20gw03fgli.jpg"><meta property="og:updated_time" content="2019-04-25T11:33:36.356Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Spark从外部数据集中读取数据"><meta name="twitter:description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据"><meta name="twitter:image" content="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg"><link rel="canonical" href="https://qiming.info/Spark从外部数据集中读取数据/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement(o),n=t.getElementsByTagName(o)[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,"script",("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"6629f244"}),daovoice("update")</script><title>Spark从外部数据集中读取数据 | QIMING.INFO</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta custom-logo"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">QIMING.INFO</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">You are more than what you have become now.</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-关于-/-留言板"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于 / 留言板</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">48</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">23</span></a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i> 公益 404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div class="posts-expand"><article itemscope="" itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://qiming.info/Spark从外部数据集中读取数据/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="Xu Qiming"><meta itemprop="description" content="软件工程硕士在读"><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="QIMING.INFO"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Spark从外部数据集中读取数据</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-06-06 11:12:24" itemprop="dateCreated datePublished" datetime="2018-06-06T11:12:24+08:00">2018-06-06</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span></span><span id="/Spark从外部数据集中读取数据/" class="post-meta-item leancloud_visitors" data-flag-title="Spark从外部数据集中读取数据" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Valine：</span><a title="valine" href="/Spark从外部数据集中读取数据/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/Spark从外部数据集中读取数据/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>本文将介绍几种从Spark中读取数据存入RDD的方式，分别是</p><ul><li>从HDFS中读数据</li><li>从MySQL数据库中读数据</li><li>从HBase数据库中读数据</li></ul><a id="more"></a><p>本文中涉及到的工具版本如下：</p><ul><li>Hadoop：2.7.4</li><li>Spark：2.1.1</li><li>HBase：1.2.6</li><li>MySQL：5.7.22</li><li>JDK：1.8.0_171</li><li>Scala：2.11.8</li></ul><h1 id="1-从HDFS中读数据"><a href="#1-从HDFS中读数据" class="headerlink" title="1 从HDFS中读数据"></a>1 从HDFS中读数据</h1><h2 id="1-1-准备数据"><a href="#1-1-准备数据" class="headerlink" title="1.1 准备数据"></a>1.1 准备数据</h2><p>首先启动Hadoop（使用<code>start-dfs.sh</code>），在HDFS上创建一个目录：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -mkdir -p /user/hadoop/input</span><br></pre></td></tr></table></figure><p></p><p>新建一个文件<code>input.txt</code>，内容如下：<br></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">15 78 89 22</span><br><span class="line">777 32 4 50</span><br></pre></td></tr></table></figure><p></p><p>将<code>input.txt</code>上传到HDFS上：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put input.txt /user/hadoop/input</span><br></pre></td></tr></table></figure><p></p><p>用ls命令查看是否上传成功：<br><img src="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg" alt="ls"></p><h2 id="1-2-读取数据"><a href="#1-2-读取数据" class="headerlink" title="1.2 读取数据"></a>1.2 读取数据</h2><p>Spark将读取到的数据会保存在RDD中，关于RDD的介绍可以参考本站的这篇文章<a href="https://qiming.info/Spark-RDD的简单使用">Spark-RDD的简单使用</a>。<br>在Spark中从HDFS读取文本文件可以使用<code>sc.textFile</code>方法，将此方法的参数设为<code>hdfs://master:port/path</code>即可。<br>所以本例中的读取步骤如下：<br>进入spark的安装目录，使用<code>bin/spark-shell</code>来启动<code>spark</code>命令行编程（语言为<code>scala</code>）。<br>输入以下代码：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"hdfs://localhost:9000/user/hadoop/input/input.txt"</span>)</span><br><span class="line">rdd.count()           <span class="comment">// 输出行数</span></span><br><span class="line">rdd.foreach(println)  <span class="comment">// 将所有内容打印出来</span></span><br></pre></td></tr></table></figure><p></p><p><img src="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcryqj20g3040glj.jpg" alt="hdfsresult"></p><h1 id="2-从MySQL数据库中读数据"><a href="#2-从MySQL数据库中读数据" class="headerlink" title="2 从MySQL数据库中读数据"></a>2 从MySQL数据库中读数据</h1><h2 id="2-1-数据来源"><a href="#2-1-数据来源" class="headerlink" title="2.1 数据来源"></a>2.1 数据来源</h2><p>将db_score数据库中的tb_course表作为数据来源，表中内容如下图：<br><img src="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszdh2kj20dm0deaah.jpg" alt="mysql"></p><h2 id="2-2-读取数据"><a href="#2-2-读取数据" class="headerlink" title="2.2 读取数据"></a>2.2 读取数据</h2><p>Spark可以用JDBC来连接关系型数据库，包括MySQL、Oracle、Postgre等系统。<br>在执行<code>spark-shell</code>或者<code>spark-submit</code>命令的时候，需在<code>--driver-class-path</code>配置对应数据库的JDBC驱动的路径。<br>本例中，使用以下命令启动spark-shell：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --driver-class-path /home/hadoop/mysql-connector-java-5.1.21-bin.jar</span><br></pre></td></tr></table></figure><p></p><h3 id="2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD"><a href="#2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD" class="headerlink" title="2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD"></a>2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD</h3><p>代码及说明如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">DriverManager</span></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">ResultSet</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createConnection</span></span>() = &#123;              <span class="comment">//创建连接</span></span><br><span class="line">  <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line"><span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/db_score"</span>,<span class="string">"root"</span>,<span class="string">"passwd"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractValues</span></span>(r:<span class="type">ResultSet</span>) = &#123;      <span class="comment">//从数据库中取得数据后转换格式</span></span><br><span class="line">  (r.getInt(<span class="number">1</span>),r.getString(<span class="number">2</span>))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> courseRdd = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(            <span class="comment">// 调用JdbcRDD类</span></span><br><span class="line">  sc,                                   <span class="comment">// SparkContext对象</span></span><br><span class="line">  createConnection,                     <span class="comment">// 与数据库的连接</span></span><br><span class="line">  <span class="string">"select * from tb_course where ? &lt;= courseid and courseid &lt;= ?"</span>, <span class="comment">// SQL语句</span></span><br><span class="line">  <span class="number">1</span>,                                    <span class="comment">// 查询的下界</span></span><br><span class="line">  <span class="number">7</span>,                                    <span class="comment">// 查询的上界</span></span><br><span class="line">  <span class="number">2</span>,                                    <span class="comment">// partition的个数(即分为几部分查询)</span></span><br><span class="line">  extractValues                         <span class="comment">// 将数据转换成需要的格式</span></span><br><span class="line">)</span><br><span class="line">courseRdd.collect.foreach(println)           <span class="comment">// 打印输出</span></span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszfc11j208p03rt8l.jpg" alt="courseRDD"></p><blockquote><p>注：从上例中可以看出，使用JdbcRDD时，SQL查询语句必须有类似<code>ID &gt;= ? AND ID &lt;= ?</code>这样的where语句（经测试，直接去掉会报错），而且上界和下界的类型必须是Long，这样使得JdbcRDD的使用场景比较局限。不过参照JdbcRDD的源代码，用户可以修改源代码以写出符合自己需求的JdbcRDD。</p></blockquote><h3 id="2-2-2-方法二：使用Spark-SQL来返回一个DataFrame"><a href="#2-2-2-方法二：使用Spark-SQL来返回一个DataFrame" class="headerlink" title="2.2.2 方法二：使用Spark SQL来返回一个DataFrame"></a>2.2.2 方法二：使用Spark SQL来返回一个DataFrame</h3><p>代码及说明如下：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line"></span><br><span class="line">val sqlContext = new SQLContext(sc)               // 生成SQLContext对象</span><br><span class="line">val sql = <span class="string">"select * from tb_course"</span>               // SQL查询语句</span><br><span class="line"></span><br><span class="line">val courseDF = sqlContext.read.format(<span class="string">"jdbc"</span>).options(</span><br><span class="line">  Map(<span class="string">"url"</span>-&gt;<span class="string">"jdbc:mysql://localhost:3306/db_score"</span>,</span><br><span class="line">    <span class="string">"dbtable"</span>-&gt;s<span class="string">"(<span class="variable">$&#123;sql&#125;</span>) as table01"</span>,            // SQL查询并对结果起别名</span><br><span class="line">    <span class="string">"driver"</span>-&gt;<span class="string">"com.mysql.jdbc.Driver"</span>,            // 驱动</span><br><span class="line">    <span class="string">"user"</span>-&gt; <span class="string">"root"</span>,                              // 用户名</span><br><span class="line">    <span class="string">"password"</span>-&gt;<span class="string">"passwd"</span>)                         // 密码</span><br><span class="line">).load()</span><br><span class="line"></span><br><span class="line">courseDF.collect().foreach(println)               // 打印输出</span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszgge9j208h03pwed.jpg" alt="courseDF"></p><h1 id="3-从HBase数据库中读数据"><a href="#3-从HBase数据库中读数据" class="headerlink" title="3 从HBase数据库中读数据"></a>3 从HBase数据库中读数据</h1><h2 id="3-1-准备数据"><a href="#3-1-准备数据" class="headerlink" title="3.1 准备数据"></a>3.1 准备数据</h2><p>首先启动HDFS（<code>start-dfs.sh</code>）和HBase（<code>start-hbase.sh</code>）<br>输入<code>hbase shell</code>进入HBase的命令行模式<br>使用create命令创建一张有f1、f2两个列族的表：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main) &gt; create <span class="string">'test1'</span>,&#123;NAME =&gt; <span class="string">'f1'</span>&#125;,&#123;NAME =&gt; <span class="string">'f2'</span>&#125;</span><br></pre></td></tr></table></figure><p></p><p>使用put命令给表<code>test1</code>添加一些测试数据：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row01'</span>,<span class="string">'f1:data'</span>,<span class="string">'10001'</span> </span><br><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row01'</span>,<span class="string">'f2:data'</span>,<span class="string">'10002'</span></span><br><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row02'</span>,<span class="string">'f2:data'</span>,<span class="string">'10003'</span></span><br></pre></td></tr></table></figure><p></p><p>查看添加的数据：<br><img src="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszck52j20kh02lq2w.jpg" alt="scantest1"></p><h2 id="3-2-读取数据"><a href="#3-2-读取数据" class="headerlink" title="3.2 读取数据"></a>3.2 读取数据</h2><p>Spark连接HBase时需要一些必要的jar包，可在HBase安装目录下的lib文件夹中找到，将它们复制到一个自定义文件夹中（本例中在Spark安装目录下新建了名为hbase-lib的文件夹），这些jar包清单如下：<br><img src="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszd8wyj20ka03y0su.jpg" alt="sparkhbasejars"><br>即metrics-core-2.2.0.jar、protobuf-java-2.5.0.jar、htrace-core-3.1.0-incubating.jar、guava-12.0.1.jar这四个jar包加上所有hbase-开头的所有jar包。（注：spark的环境中有metrics的jar包，但是可能是版本不匹配的问题，如果不加入此2.2.0版本的，程序会报错）<br>然后在Spark安装目录下的conf文件夹中找到<code>spark-env.sh</code>,在其中添加：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_CLASSPATH=/opt/software/spark/hbase-lib/*</span><br></pre></td></tr></table></figure><p></p><h3 id="3-2-1-方法一：调用newAPIHadoopRDD"><a href="#3-2-1-方法一：调用newAPIHadoopRDD" class="headerlink" title="3.2.1 方法一：调用newAPIHadoopRDD"></a>3.2.1 方法一：调用<code>newAPIHadoopRDD</code></h3><p>代码及相关说明如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.<span class="type">Result</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>,<span class="string">"test1"</span>)   <span class="comment">//设置需要扫描的表(test1)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sc.newAPIHadoopRDD(conf,</span><br><span class="line">classOf[<span class="type">TableInputFormat</span>],classOf[<span class="type">ImmutableBytesWritable</span>],classOf[<span class="type">Result</span>])</span><br></pre></td></tr></table></figure><p></p><p>由于<code>TableInputFormat</code>类的实现，Spark可以用Hadoop输入格式访问HBase，即调用<code>sc.newAPIHadoopRDD</code>，此方法返回一个键值对类型的RDD，其中键的类型为<code>ImmutableBytesWritable</code>，值的类型为<code>Result</code>（分别是此方法的后两个参数）。<br>因此，遍历此键值对RDD中的值即可取得想要的数据，代码如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach&#123;<span class="keyword">case</span> (_,result) =&gt;&#123;              <span class="comment">//逐行遍历</span></span><br><span class="line">  <span class="keyword">val</span> row = <span class="type">Bytes</span>.toString(result.getRow)    <span class="comment">//获取当前行的Row key</span></span><br><span class="line">  <span class="keyword">val</span> value = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"f2"</span>.getBytes,<span class="string">"data"</span>.getBytes))</span><br><span class="line">                            <span class="comment">//根据列族名(f2)和列名(data)取当前行的数据</span></span><br><span class="line">  println(<span class="string">"Row:"</span>+row+<span class="string">" f2, data:"</span>+value)     <span class="comment">//打印输出</span></span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure><p></p><p>运行结果如下：<br><img src="http://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqjlj20gw03fgli.jpg" alt=""></p><h3 id="3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法"><a href="#3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法" class="headerlink" title="3.2.2 方法二：用org.apache.hadoop.hbase中提供的方法"></a>3.2.2 方法二：用<code>org.apache.hadoop.hbase</code>中提供的方法</h3><p>以下代码改编自《Hadoop+Spark生态系统操作与实战指南》，利用此代码可以实现对HBase的CRUD操作，代码如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client._</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;<span class="type">HBaseConfiguration</span>, <span class="type">HColumnDescriptor</span>,</span><br><span class="line"><span class="type">HTableDescriptor</span>, <span class="type">TableName</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createHTable</span></span>(connection: <span class="type">Connection</span>,tablename: <span class="type">String</span>): <span class="type">Unit</span>=</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//Hbase表模式管理器</span></span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="comment">//本例将操作的表名</span></span><br><span class="line">  <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">  <span class="comment">//如果需要创建表</span></span><br><span class="line">  <span class="keyword">if</span> (!admin.tableExists(tableName)) &#123;</span><br><span class="line">    <span class="comment">//创建Hbase表模式</span></span><br><span class="line">    <span class="keyword">val</span> tableDescriptor = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(tableName)</span><br><span class="line">    <span class="comment">//创建列簇1    artitle</span></span><br><span class="line">    tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"artitle"</span>.getBytes()))</span><br><span class="line">    <span class="comment">//创建列簇2    author</span></span><br><span class="line">    tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"author"</span>.getBytes()))</span><br><span class="line">    <span class="comment">//创建表</span></span><br><span class="line">    admin.createTable(tableDescriptor)</span><br><span class="line">    println(<span class="string">"create done."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//删除表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteHTable</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="comment">//本例将操作的表名</span></span><br><span class="line">  <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">  <span class="comment">//Hbase表模式管理器</span></span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(tableName))&#123;</span><br><span class="line">    admin.disableTable(tableName)</span><br><span class="line">    admin.deleteTable(tableName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertHTable</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>,value:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    <span class="keyword">val</span> table=connection.getTable(userTable)</span><br><span class="line">    <span class="comment">//准备key 的数据</span></span><br><span class="line">    <span class="keyword">val</span> p=<span class="keyword">new</span> <span class="type">Put</span>(key.getBytes)</span><br><span class="line">    <span class="comment">//为put操作指定 column 和 value</span></span><br><span class="line">    p.addColumn(family.getBytes,column.getBytes,value.getBytes())</span><br><span class="line">    <span class="comment">//提交一行</span></span><br><span class="line">    table.put(p)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//基于KEY查询某条数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAResult</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> g=<span class="keyword">new</span> <span class="type">Get</span>(key.getBytes())</span><br><span class="line">    <span class="keyword">val</span> result=table.get(g)</span><br><span class="line">    <span class="keyword">val</span> value=<span class="type">Bytes</span>.toString(result.getValue(family.getBytes(),column.getBytes()))</span><br><span class="line">    println(<span class="string">"value:"</span>+value)</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)table.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除某条记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteRecord</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable=<span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> d=<span class="keyword">new</span> <span class="type">Delete</span>(key.getBytes())</span><br><span class="line">    d.addColumn(family.getBytes(),column.getBytes())</span><br><span class="line">    table.delete(d)</span><br><span class="line">    println(<span class="string">"delete record done."</span>)</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)table.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanRecord</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> scanner:<span class="type">ResultScanner</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable=<span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> s=<span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">    s.addColumn(family.getBytes(),column.getBytes())</span><br><span class="line">    scanner=table.getScanner(s)</span><br><span class="line">    println(<span class="string">"scan...for..."</span>)</span><br><span class="line">    <span class="keyword">var</span> result:<span class="type">Result</span>=scanner.next()</span><br><span class="line">    <span class="keyword">while</span>(result!=<span class="literal">null</span>)&#123;</span><br><span class="line">      println(<span class="string">"Found row:"</span> + result)</span><br><span class="line">      println(<span class="string">"Found value: "</span>+</span><br><span class="line"><span class="type">Bytes</span>.toString(result.getValue(family.getBytes(),column.getBytes())))</span><br><span class="line">      result=scanner.next()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)</span><br><span class="line">      table.close()</span><br><span class="line">    scanner.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>（注：以上代码中的Key均代表<code>Row Key</code>）<br>以上代码将在HBase中创建表、删除表、插入记录、根据行号查询数据、删除记录、扫描记录等操作都写成了函数，将以上代码在spark-shell中运行后，对HBase的操作直接调用相关函数即可，如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个配置，采用的是工厂方法</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create</span><br><span class="line"><span class="comment">//Connection 的创建是个重量级的工作，线程安全，是操作hbase的入口</span></span><br><span class="line"><span class="keyword">val</span> connection= <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表测试</span></span><br><span class="line">createHTable(connection, <span class="string">"HadoopAndSpark"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入数据,重复执行为覆盖</span></span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"002"</span>,<span class="string">"Hadoop for me"</span>)</span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"003"</span>,<span class="string">"Java for me"</span>)</span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Spark"</span>,<span class="string">"002"</span>,<span class="string">"Scala for me"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除记录</span></span><br><span class="line">deleteRecord(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Spark"</span>,<span class="string">"002"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描整个表</span></span><br><span class="line">scanRecord(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据表名、行键、列族、列名取当前Cell的数据</span></span><br><span class="line">getAResult(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"002"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除表测试</span></span><br><span class="line">deleteHTable(connection, <span class="string">"HadoopAndSpark"</span>)</span><br></pre></td></tr></table></figure><p></p><h1 id="4-后记"><a href="#4-后记" class="headerlink" title="4 后记"></a>4 后记</h1><p>Spark可以通过所有Hadoop支持的外部数据源（包括本地文件系统、HDFS、Cassandra、关系型数据库、HBase、亚马逊S3等）建立RDD，本文没有讲到的，后续视情况补充。Spark支持文本文件、序列文件及其他任何Hadoop输入格式文件。</p><h1 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5 参考资料"></a>5 参考资料</h1><p>[1]Karau,H.&amp;A.Konwinski.Spark快速大数据分析[M].王道远译.北京:人民邮电出版社.2015-09:64-65,81-85<br>[2]余辉.Hadoop+Spark生态系统操作与实战指南[M].北京:清华大学出版社.2017:136-140</p></div><div id="reward-container"><div><div>-----本文结束<i class="fa fa-smile-o"></i>感谢您的阅读-----<hr>如我有幸帮到了您，那么，不妨<i class="far fa-hand-point-down"></i><i class="far fa-hand-point-down"></i><i class="far fa-hand-point-down"></i>~~~谢谢！</div></div> <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Xu Qiming 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="Xu Qiming 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Xu Qiming</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://qiming.info/Spark从外部数据集中读取数据/" title="Spark从外部数据集中读取数据">https://qiming.info/Spark从外部数据集中读取数据/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/Spark/" rel="tag"># Spark</a> <a href="/tags/HDFS/" rel="tag"># HDFS</a> <a href="/tags/RDD/" rel="tag"># RDD</a> <a href="/tags/Hadoop/" rel="tag"># Hadoop</a> <a href="/tags/HBase/" rel="tag"># HBase</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Xv6学习小计1/" rel="next" title="Xv6学习小记（一）——编译与运行"><i class="fa fa-chevron-left"></i> Xv6学习小记（一）——编译与运行</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/Spark-RDD的简单使用/" rel="prev" title="Spark RDD的简单使用">Spark RDD的简单使用<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="comments"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-从HDFS中读数据"><span class="nav-text">1 从HDFS中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-准备数据"><span class="nav-text">1.1 准备数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-读取数据"><span class="nav-text">1.2 读取数据</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-从MySQL数据库中读数据"><span class="nav-text">2 从MySQL数据库中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-数据来源"><span class="nav-text">2.1 数据来源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-读取数据"><span class="nav-text">2.2 读取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD"><span class="nav-text">2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-方法二：使用Spark-SQL来返回一个DataFrame"><span class="nav-text">2.2.2 方法二：使用Spark SQL来返回一个DataFrame</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-从HBase数据库中读数据"><span class="nav-text">3 从HBase数据库中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-准备数据"><span class="nav-text">3.1 准备数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-读取数据"><span class="nav-text">3.2 读取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-方法一：调用newAPIHadoopRDD"><span class="nav-text">3.2.1 方法一：调用newAPIHadoopRDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法"><span class="nav-text">3.2.2 方法二：用org.apache.hadoop.hbase中提供的方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-后记"><span class="nav-text">4 后记</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-参考资料"><span class="nav-text">5 参考资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Xu Qiming"><p class="site-author-name" itemprop="name">Xu Qiming</p><div class="site-description" itemprop="description">软件工程硕士在读</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">23</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/Scorpio-xu" title="GitHub &rarr; https://github.com/Scorpio-xu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/xqming" title="知乎 &rarr; https://www.zhihu.com/people/xqming" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/u/3020746395" title="微博 &rarr; https://weibo.com/u/3020746395" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i> 微博</a></span><span class="links-of-author-item"><a href="mailto:xqmcode@gmail.com" title="E-Mail &rarr; mailto:xqmcode@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2019</span><span class="with-love" id="animate"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Xu Qiming</span></div><div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.4.1</div></div></footer></div><script src="/lib/anime.min.js?v=3.1.0"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script><script src="/js/schemes/muse.js?v=7.4.1"></script><script src="/js/next-boot.js?v=7.4.1"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js?v=7.4.1"></script><script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: true,
    appId: 'YFNNFJwKUBIii7YuKvisDgx5-gzGzoHsz',
    appKey: 'dQtRYG8ToS3H5xs4c0L2sQtW',
    placeholder: '评论前输入您的邮箱能收到回复提醒哦~',
    avatar: 'retro',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: ,
    serverURLs: ''
  });
}, window.Valine);
</script></body></html>