<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="description" content="Spark从外部数据集中读取数据"><meta name="keywords" content="Spark,HDFS,RDD,Hadoop,HBase"><meta name="author" content="Xu Qiming,undefined"><meta name="copyright" content="Xu Qiming"><title>Spark从外部数据集中读取数据 | QIMING.INFO</title><link rel="shortcut icon" href="/my-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容:${query}"}},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"}}</script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-从HDFS中读数据"><span class="toc-text">1 从HDFS中读数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-准备数据"><span class="toc-text">1.1 准备数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-读取数据"><span class="toc-text">1.2 读取数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-从MySQL数据库中读数据"><span class="toc-text">2 从MySQL数据库中读数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-数据来源"><span class="toc-text">2.1 数据来源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-读取数据"><span class="toc-text">2.2 读取数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD"><span class="toc-text">2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-方法二：使用Spark-SQL来返回一个DataFrame"><span class="toc-text">2.2.2 方法二：使用Spark SQL来返回一个DataFrame</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-从HBase数据库中读数据"><span class="toc-text">3 从HBase数据库中读数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-准备数据"><span class="toc-text">3.1 准备数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-读取数据"><span class="toc-text">3.2 读取数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-方法一：调用newAPIHadoopRDD"><span class="toc-text">3.2.1 方法一：调用newAPIHadoopRDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法"><span class="toc-text">3.2.2 方法二：用org.apache.hadoop.hbase中提供的方法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-后记"><span class="toc-text">4 后记</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-参考资料"><span class="toc-text">5 参考资料</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fx6sa0kwotj205k05nglk.jpg"></div><div class="author-info__name text-center">Xu Qiming</div><div class="author-info__description text-center">软件工程硕士在读</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">22</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">46</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">8</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container" style="background-image:url(true)"><div id="page-header"> <span class="pull-left"><a id="site-name" href="/">QIMING.INFO</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i> <span>搜索</span></a><a class="site-page" href="/">首页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/about">关于 / 留言板</a></span></div><div id="post-info"><div id="post-title">Spark从外部数据集中读取数据</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-06-06</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/大数据/">大数据</a><div class="post-meta-wordcount"><span>字数总计:</span> <span class="word-count">2.2k</span><span class="post-meta__separator">|</span><span>阅读时长: 9 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本文将介绍几种从Spark中读取数据存入RDD的方式，分别是</p><ul><li>从HDFS中读数据</li><li>从MySQL数据库中读数据</li><li>从HBase数据库中读数据</li></ul><a id="more"></a><p>本文中涉及到的工具版本如下：</p><ul><li>Hadoop：2.7.4</li><li>Spark：2.1.1</li><li>HBase：1.2.6</li><li>MySQL：5.7.22</li><li>JDK：1.8.0_171</li><li>Scala：2.11.8</li></ul><h1 id="1-从HDFS中读数据"><a href="#1-从HDFS中读数据" class="headerlink" title="1 从HDFS中读数据"></a>1 从HDFS中读数据</h1><h2 id="1-1-准备数据"><a href="#1-1-准备数据" class="headerlink" title="1.1 准备数据"></a>1.1 准备数据</h2><p>首先启动Hadoop（使用<code>start-dfs.sh</code>），在HDFS上创建一个目录：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -mkdir -p /user/hadoop/input</span><br></pre></td></tr></table></figure><p></p><p>新建一个文件<code>input.txt</code>，内容如下：<br></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">15 78 89 22</span><br><span class="line">777 32 4 50</span><br></pre></td></tr></table></figure><p></p><p>将<code>input.txt</code>上传到HDFS上：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put input.txt /user/hadoop/input</span><br></pre></td></tr></table></figure><p></p><p>用ls命令查看是否上传成功：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg" alt="ls"></p><h2 id="1-2-读取数据"><a href="#1-2-读取数据" class="headerlink" title="1.2 读取数据"></a>1.2 读取数据</h2><p>Spark将读取到的数据会保存在RDD中，关于RDD的介绍可以参考本站的这篇文章<a href="https://qiming.info/Spark-RDD的简单使用">Spark-RDD的简单使用</a>。<br>在Spark中从HDFS读取文本文件可以使用<code>sc.textFile</code>方法，将此方法的参数设为<code>hdfs://master:port/path</code>即可。<br>所以本例中的读取步骤如下：<br>进入spark的安装目录，使用<code>bin/spark-shell</code>来启动<code>spark</code>命令行编程（语言为<code>scala</code>）。<br>输入以下代码：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"hdfs://localhost:9000/user/hadoop/input/input.txt"</span>)</span><br><span class="line">rdd.count()           <span class="comment">// 输出行数</span></span><br><span class="line">rdd.foreach(println)  <span class="comment">// 将所有内容打印出来</span></span><br></pre></td></tr></table></figure><p></p><p><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcryqj20g3040glj.jpg" alt="hdfsresult"></p><h1 id="2-从MySQL数据库中读数据"><a href="#2-从MySQL数据库中读数据" class="headerlink" title="2 从MySQL数据库中读数据"></a>2 从MySQL数据库中读数据</h1><h2 id="2-1-数据来源"><a href="#2-1-数据来源" class="headerlink" title="2.1 数据来源"></a>2.1 数据来源</h2><p>将db_score数据库中的tb_course表作为数据来源，表中内容如下图：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszdh2kj20dm0deaah.jpg" alt="mysql"></p><h2 id="2-2-读取数据"><a href="#2-2-读取数据" class="headerlink" title="2.2 读取数据"></a>2.2 读取数据</h2><p>Spark可以用JDBC来连接关系型数据库，包括MySQL、Oracle、Postgre等系统。<br>在执行<code>spark-shell</code>或者<code>spark-submit</code>命令的时候，需在<code>--driver-class-path</code>配置对应数据库的JDBC驱动的路径。<br>本例中，使用以下命令启动spark-shell：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --driver-class-path /home/hadoop/mysql-connector-java-5.1.21-bin.jar</span><br></pre></td></tr></table></figure><p></p><h3 id="2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD"><a href="#2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD" class="headerlink" title="2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD"></a>2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD</h3><p>代码及说明如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">DriverManager</span></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">ResultSet</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createConnection</span></span>() = &#123;              <span class="comment">//创建连接</span></span><br><span class="line">  <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line"><span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/db_score"</span>,<span class="string">"root"</span>,<span class="string">"passwd"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractValues</span></span>(r:<span class="type">ResultSet</span>) = &#123;      <span class="comment">//从数据库中取得数据后转换格式</span></span><br><span class="line">  (r.getInt(<span class="number">1</span>),r.getString(<span class="number">2</span>))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> courseRdd = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(            <span class="comment">// 调用JdbcRDD类</span></span><br><span class="line">  sc,                                   <span class="comment">// SparkContext对象</span></span><br><span class="line">  createConnection,                     <span class="comment">// 与数据库的连接</span></span><br><span class="line">  <span class="string">"select * from tb_course where ? &lt;= courseid and courseid &lt;= ?"</span>, <span class="comment">// SQL语句</span></span><br><span class="line">  <span class="number">1</span>,                                    <span class="comment">// 查询的下界</span></span><br><span class="line">  <span class="number">7</span>,                                    <span class="comment">// 查询的上界</span></span><br><span class="line">  <span class="number">2</span>,                                    <span class="comment">// partition的个数(即分为几部分查询)</span></span><br><span class="line">  extractValues                         <span class="comment">// 将数据转换成需要的格式</span></span><br><span class="line">)</span><br><span class="line">courseRdd.collect.foreach(println)           <span class="comment">// 打印输出</span></span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszfc11j208p03rt8l.jpg" alt="courseRDD"></p><blockquote><p>注：从上例中可以看出，使用JdbcRDD时，SQL查询语句必须有类似<code>ID &gt;= ? AND ID &lt;= ?</code>这样的where语句（经测试，直接去掉会报错），而且上界和下界的类型必须是Long，这样使得JdbcRDD的使用场景比较局限。不过参照JdbcRDD的源代码，用户可以修改源代码以写出符合自己需求的JdbcRDD。</p></blockquote><h3 id="2-2-2-方法二：使用Spark-SQL来返回一个DataFrame"><a href="#2-2-2-方法二：使用Spark-SQL来返回一个DataFrame" class="headerlink" title="2.2.2 方法二：使用Spark SQL来返回一个DataFrame"></a>2.2.2 方法二：使用Spark SQL来返回一个DataFrame</h3><p>代码及说明如下：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line"></span><br><span class="line">val sqlContext = new SQLContext(sc)               // 生成SQLContext对象</span><br><span class="line">val sql = <span class="string">"select * from tb_course"</span>               // SQL查询语句</span><br><span class="line"></span><br><span class="line">val courseDF = sqlContext.read.format(<span class="string">"jdbc"</span>).options(</span><br><span class="line">  Map(<span class="string">"url"</span>-&gt;<span class="string">"jdbc:mysql://localhost:3306/db_score"</span>,</span><br><span class="line">    <span class="string">"dbtable"</span>-&gt;s<span class="string">"(<span class="variable">$&#123;sql&#125;</span>) as table01"</span>,            // SQL查询并对结果起别名</span><br><span class="line">    <span class="string">"driver"</span>-&gt;<span class="string">"com.mysql.jdbc.Driver"</span>,            // 驱动</span><br><span class="line">    <span class="string">"user"</span>-&gt; <span class="string">"root"</span>,                              // 用户名</span><br><span class="line">    <span class="string">"password"</span>-&gt;<span class="string">"passwd"</span>)                         // 密码</span><br><span class="line">).load()</span><br><span class="line"></span><br><span class="line">courseDF.collect().foreach(println)               // 打印输出</span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszgge9j208h03pwed.jpg" alt="courseDF"></p><h1 id="3-从HBase数据库中读数据"><a href="#3-从HBase数据库中读数据" class="headerlink" title="3 从HBase数据库中读数据"></a>3 从HBase数据库中读数据</h1><h2 id="3-1-准备数据"><a href="#3-1-准备数据" class="headerlink" title="3.1 准备数据"></a>3.1 准备数据</h2><p>首先启动HDFS（<code>start-dfs.sh</code>）和HBase（<code>start-hbase.sh</code>）<br>输入<code>hbase shell</code>进入HBase的命令行模式<br>使用create命令创建一张有f1、f2两个列族的表：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main) &gt; create <span class="string">'test1'</span>,&#123;NAME =&gt; <span class="string">'f1'</span>&#125;,&#123;NAME =&gt; <span class="string">'f2'</span>&#125;</span><br></pre></td></tr></table></figure><p></p><p>使用put命令给表<code>test1</code>添加一些测试数据：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row01'</span>,<span class="string">'f1:data'</span>,<span class="string">'10001'</span> </span><br><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row01'</span>,<span class="string">'f2:data'</span>,<span class="string">'10002'</span></span><br><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row02'</span>,<span class="string">'f2:data'</span>,<span class="string">'10003'</span></span><br></pre></td></tr></table></figure><p></p><p>查看添加的数据：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszck52j20kh02lq2w.jpg" alt="scantest1"></p><h2 id="3-2-读取数据"><a href="#3-2-读取数据" class="headerlink" title="3.2 读取数据"></a>3.2 读取数据</h2><p>Spark连接HBase时需要一些必要的jar包，可在HBase安装目录下的lib文件夹中找到，将它们复制到一个自定义文件夹中（本例中在Spark安装目录下新建了名为hbase-lib的文件夹），这些jar包清单如下：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszd8wyj20ka03y0su.jpg" alt="sparkhbasejars"><br>即metrics-core-2.2.0.jar、protobuf-java-2.5.0.jar、htrace-core-3.1.0-incubating.jar、guava-12.0.1.jar这四个jar包加上所有hbase-开头的所有jar包。（注：spark的环境中有metrics的jar包，但是可能是版本不匹配的问题，如果不加入此2.2.0版本的，程序会报错）<br>然后在Spark安装目录下的conf文件夹中找到<code>spark-env.sh</code>,在其中添加：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_CLASSPATH=/opt/software/spark/hbase-lib/*</span><br></pre></td></tr></table></figure><p></p><h3 id="3-2-1-方法一：调用newAPIHadoopRDD"><a href="#3-2-1-方法一：调用newAPIHadoopRDD" class="headerlink" title="3.2.1 方法一：调用newAPIHadoopRDD"></a>3.2.1 方法一：调用<code>newAPIHadoopRDD</code></h3><p>代码及相关说明如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.<span class="type">Result</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>,<span class="string">"test1"</span>)   <span class="comment">//设置需要扫描的表(test1)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sc.newAPIHadoopRDD(conf,</span><br><span class="line">classOf[<span class="type">TableInputFormat</span>],classOf[<span class="type">ImmutableBytesWritable</span>],classOf[<span class="type">Result</span>])</span><br></pre></td></tr></table></figure><p></p><p>由于<code>TableInputFormat</code>类的实现，Spark可以用Hadoop输入格式访问HBase，即调用<code>sc.newAPIHadoopRDD</code>，此方法返回一个键值对类型的RDD，其中键的类型为<code>ImmutableBytesWritable</code>，值的类型为<code>Result</code>（分别是此方法的后两个参数）。<br>因此，遍历此键值对RDD中的值即可取得想要的数据，代码如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach&#123;<span class="keyword">case</span> (_,result) =&gt;&#123;              <span class="comment">//逐行遍历</span></span><br><span class="line">  <span class="keyword">val</span> row = <span class="type">Bytes</span>.toString(result.getRow)    <span class="comment">//获取当前行的Row key</span></span><br><span class="line">  <span class="keyword">val</span> value = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"f2"</span>.getBytes,<span class="string">"data"</span>.getBytes))</span><br><span class="line">                            <span class="comment">//根据列族名(f2)和列名(data)取当前行的数据</span></span><br><span class="line">  println(<span class="string">"Row:"</span>+row+<span class="string">" f2, data:"</span>+value)     <span class="comment">//打印输出</span></span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure><p></p><p>运行结果如下：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqjlj20gw03fgli.jpg" alt=""></p><h3 id="3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法"><a href="#3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法" class="headerlink" title="3.2.2 方法二：用org.apache.hadoop.hbase中提供的方法"></a>3.2.2 方法二：用<code>org.apache.hadoop.hbase</code>中提供的方法</h3><p>以下代码改编自《Hadoop+Spark生态系统操作与实战指南》，利用此代码可以实现对HBase的CRUD操作，代码如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client._</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;<span class="type">HBaseConfiguration</span>, <span class="type">HColumnDescriptor</span>,</span><br><span class="line"><span class="type">HTableDescriptor</span>, <span class="type">TableName</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createHTable</span></span>(connection: <span class="type">Connection</span>,tablename: <span class="type">String</span>): <span class="type">Unit</span>=</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//Hbase表模式管理器</span></span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="comment">//本例将操作的表名</span></span><br><span class="line">  <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">  <span class="comment">//如果需要创建表</span></span><br><span class="line">  <span class="keyword">if</span> (!admin.tableExists(tableName)) &#123;</span><br><span class="line">    <span class="comment">//创建Hbase表模式</span></span><br><span class="line">    <span class="keyword">val</span> tableDescriptor = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(tableName)</span><br><span class="line">    <span class="comment">//创建列簇1    artitle</span></span><br><span class="line">    tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"artitle"</span>.getBytes()))</span><br><span class="line">    <span class="comment">//创建列簇2    author</span></span><br><span class="line">    tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"author"</span>.getBytes()))</span><br><span class="line">    <span class="comment">//创建表</span></span><br><span class="line">    admin.createTable(tableDescriptor)</span><br><span class="line">    println(<span class="string">"create done."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//删除表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteHTable</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="comment">//本例将操作的表名</span></span><br><span class="line">  <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">  <span class="comment">//Hbase表模式管理器</span></span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(tableName))&#123;</span><br><span class="line">    admin.disableTable(tableName)</span><br><span class="line">    admin.deleteTable(tableName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertHTable</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>,value:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    <span class="keyword">val</span> table=connection.getTable(userTable)</span><br><span class="line">    <span class="comment">//准备key 的数据</span></span><br><span class="line">    <span class="keyword">val</span> p=<span class="keyword">new</span> <span class="type">Put</span>(key.getBytes)</span><br><span class="line">    <span class="comment">//为put操作指定 column 和 value</span></span><br><span class="line">    p.addColumn(family.getBytes,column.getBytes,value.getBytes())</span><br><span class="line">    <span class="comment">//提交一行</span></span><br><span class="line">    table.put(p)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//基于KEY查询某条数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAResult</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> g=<span class="keyword">new</span> <span class="type">Get</span>(key.getBytes())</span><br><span class="line">    <span class="keyword">val</span> result=table.get(g)</span><br><span class="line">    <span class="keyword">val</span> value=<span class="type">Bytes</span>.toString(result.getValue(family.getBytes(),column.getBytes()))</span><br><span class="line">    println(<span class="string">"value:"</span>+value)</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)table.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除某条记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteRecord</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable=<span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> d=<span class="keyword">new</span> <span class="type">Delete</span>(key.getBytes())</span><br><span class="line">    d.addColumn(family.getBytes(),column.getBytes())</span><br><span class="line">    table.delete(d)</span><br><span class="line">    println(<span class="string">"delete record done."</span>)</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)table.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanRecord</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> scanner:<span class="type">ResultScanner</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable=<span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> s=<span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">    s.addColumn(family.getBytes(),column.getBytes())</span><br><span class="line">    scanner=table.getScanner(s)</span><br><span class="line">    println(<span class="string">"scan...for..."</span>)</span><br><span class="line">    <span class="keyword">var</span> result:<span class="type">Result</span>=scanner.next()</span><br><span class="line">    <span class="keyword">while</span>(result!=<span class="literal">null</span>)&#123;</span><br><span class="line">      println(<span class="string">"Found row:"</span> + result)</span><br><span class="line">      println(<span class="string">"Found value: "</span>+</span><br><span class="line"><span class="type">Bytes</span>.toString(result.getValue(family.getBytes(),column.getBytes())))</span><br><span class="line">      result=scanner.next()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)</span><br><span class="line">      table.close()</span><br><span class="line">    scanner.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>（注：以上代码中的Key均代表<code>Row Key</code>）<br>以上代码将在HBase中创建表、删除表、插入记录、根据行号查询数据、删除记录、扫描记录等操作都写成了函数，将以上代码在spark-shell中运行后，对HBase的操作直接调用相关函数即可，如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个配置，采用的是工厂方法</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create</span><br><span class="line"><span class="comment">//Connection 的创建是个重量级的工作，线程安全，是操作hbase的入口</span></span><br><span class="line"><span class="keyword">val</span> connection= <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表测试</span></span><br><span class="line">createHTable(connection, <span class="string">"HadoopAndSpark"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入数据,重复执行为覆盖</span></span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"002"</span>,<span class="string">"Hadoop for me"</span>)</span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"003"</span>,<span class="string">"Java for me"</span>)</span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Spark"</span>,<span class="string">"002"</span>,<span class="string">"Scala for me"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除记录</span></span><br><span class="line">deleteRecord(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Spark"</span>,<span class="string">"002"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描整个表</span></span><br><span class="line">scanRecord(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据表名、行键、列族、列名取当前Cell的数据</span></span><br><span class="line">getAResult(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"002"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除表测试</span></span><br><span class="line">deleteHTable(connection, <span class="string">"HadoopAndSpark"</span>)</span><br></pre></td></tr></table></figure><p></p><h1 id="4-后记"><a href="#4-后记" class="headerlink" title="4 后记"></a>4 后记</h1><p>Spark可以通过所有Hadoop支持的外部数据源（包括本地文件系统、HDFS、Cassandra、关系型数据库、HBase、亚马逊S3等）建立RDD，本文没有讲到的，后续视情况补充。Spark支持文本文件、序列文件及其他任何Hadoop输入格式文件。</p><h1 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5 参考资料"></a>5 参考资料</h1><p>[1]Karau,H.&amp;A.Konwinski.Spark快速大数据分析[M].王道远译.北京:人民邮电出版社.2015-09:64-65,81-85<br>[2]余辉.Hadoop+Spark生态系统操作与实战指南[M].北京:清华大学出版社.2017:136-140</p></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined">Xu Qiming</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://qiming.info/Spark从外部数据集中读取数据/">https://qiming.info/Spark从外部数据集中读取数据/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://qiming.info" target="_blank">QIMING.INFO</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a><a class="post-meta__tags" href="/tags/HDFS/">HDFS</a><a class="post-meta__tags" href="/tags/RDD/">RDD</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/HBase/">HBase</a></div><div class="social-share" data-disabled="google,tencent,linkedin,diandian,facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/Spark-RDD的简单使用/"><i class="fa fa-chevron-left"></i> <span>Spark RDD的简单使用</span></a></div><div class="next-post pull-right"><a href="/Xv6学习小计1/"><span>Xv6学习小记（一）——编译与运行</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.1.9-beta9/dist/Valine.min.js"></script><script>var notify=!1,verify=!1,GUEST_INFO=["nick","mail","link"],guest_info="nick,mail,link".split(",").filter(function(i){return-1<GUEST_INFO.indexOf(i)});guest_info=0==guest_info.length?GUEST_INFO:guest_info,window.valine=new Valine({el:"#vcomment",notify:notify,verify:verify,appId:"",appKey:"",placeholder:"评论前输入您的邮箱能收到回复提醒哦~",avatar:"retro",guest_info:guest_info,pageSize:"10"})</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By Xu Qiming</div><div class="framework-info"><span>驱动 -</span> <a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 -</span> <a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://qiming.info">blog</a>!</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49b1f5">hexo-generator-search</a> <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>