<!DOCTYPE html><html class="theme-next muse use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.5.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.5.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.5.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.5.0"><link rel="mask-icon" href="/images/logo.svg?v=6.5.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"6.5.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据"><meta name="keywords" content="Spark,HDFS,RDD,Hadoop,HBase"><meta property="og:type" content="article"><meta property="og:title" content="Spark从外部数据集中读取数据"><meta property="og:url" content="https://qiming.info/Spark从外部数据集中读取数据/index.html"><meta property="og:site_name" content="QIMING.INFO"><meta property="og:description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg"><meta property="og:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcryqj20g3040glj.jpg"><meta property="og:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszdh2kj20dm0deaah.jpg"><meta property="og:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszfc11j208p03rt8l.jpg"><meta property="og:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszgge9j208h03pwed.jpg"><meta property="og:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszck52j20kh02lq2w.jpg"><meta property="og:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszd8wyj20ka03y0su.jpg"><meta property="og:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqjlj20gw03fgli.jpg"><meta property="og:updated_time" content="2018-11-18T14:39:34.096Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Spark从外部数据集中读取数据"><meta name="twitter:description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据"><meta name="twitter:image" content="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg"><link rel="alternate" href="/atom.xml" title="QIMING.INFO" type="application/atom+xml"><link rel="canonical" href="https://qiming.info/Spark从外部数据集中读取数据/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>Spark从外部数据集中读取数据 | QIMING.INFO</title><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">QIMING.INFO</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">You are more than what you have become now.</p></div><div class="site-nav-toggle"> <button aria-label="Toggle navigation bar"><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Startseite</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archiv</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://qiming.info/Spark从外部数据集中读取数据/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="Xu Qiming"><meta itemprop="description" content="软件工程硕士在读"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="QIMING.INFO"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Spark从外部数据集中读取数据</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">Veröffentlicht am</span> <time title="Post created: 2018-06-06 11:12:24" itemprop="dateCreated datePublished" datetime="2018-06-06T11:12:24+08:00">2018-06-06</time> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">Edited on</span> <time title="Updated at: 2018-11-18 22:39:34" itemprop="dateModified" datetime="2018-11-18T22:39:34+08:00">2018-11-18</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">in</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>本文将介绍几种从Spark中读取数据存入RDD的方式，分别是</p><ul><li>从HDFS中读数据</li><li>从MySQL数据库中读数据</li><li>从HBase数据库中读数据</li></ul><a id="more"></a><p>本文中涉及到的工具版本如下：</p><ul><li>Hadoop：2.7.4</li><li>Spark：2.1.1</li><li>HBase：1.2.6</li><li>MySQL：5.7.22</li><li>JDK：1.8.0_171</li><li>Scala：2.11.8</li></ul><h1 id="1-从HDFS中读数据"><a href="#1-从HDFS中读数据" class="headerlink" title="1 从HDFS中读数据"></a>1 从HDFS中读数据</h1><h2 id="1-1-准备数据"><a href="#1-1-准备数据" class="headerlink" title="1.1 准备数据"></a>1.1 准备数据</h2><p>首先启动Hadoop（使用<code>start-dfs.sh</code>），在HDFS上创建一个目录：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -mkdir -p /user/hadoop/input</span><br></pre></td></tr></table></figure><p></p><p>新建一个文件<code>input.txt</code>，内容如下：<br></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">15 78 89 22</span><br><span class="line">777 32 4 50</span><br></pre></td></tr></table></figure><p></p><p>将<code>input.txt</code>上传到HDFS上：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put input.txt /user/hadoop/input</span><br></pre></td></tr></table></figure><p></p><p>用ls命令查看是否上传成功：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg" alt="ls"></p><h2 id="1-2-读取数据"><a href="#1-2-读取数据" class="headerlink" title="1.2 读取数据"></a>1.2 读取数据</h2><p>Spark将读取到的数据会保存在RDD中，关于RDD的介绍可以参考本站的这篇文章<a href="https://qiming.info/Spark-RDD的简单使用">Spark-RDD的简单使用</a>。<br>在Spark中从HDFS读取文本文件可以使用<code>sc.textFile</code>方法，将此方法的参数设为<code>hdfs://master:port/path</code>即可。<br>所以本例中的读取步骤如下：<br>进入spark的安装目录，使用<code>bin/spark-shell</code>来启动<code>spark</code>命令行编程（语言为<code>scala</code>）。<br>输入以下代码：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"hdfs://localhost:9000/user/hadoop/input/input.txt"</span>)</span><br><span class="line">rdd.count()           <span class="comment">// 输出行数</span></span><br><span class="line">rdd.foreach(println)  <span class="comment">// 将所有内容打印出来</span></span><br></pre></td></tr></table></figure><p></p><p><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcryqj20g3040glj.jpg" alt="hdfsresult"></p><h1 id="2-从MySQL数据库中读数据"><a href="#2-从MySQL数据库中读数据" class="headerlink" title="2 从MySQL数据库中读数据"></a>2 从MySQL数据库中读数据</h1><h2 id="2-1-数据来源"><a href="#2-1-数据来源" class="headerlink" title="2.1 数据来源"></a>2.1 数据来源</h2><p>将db_score数据库中的tb_course表作为数据来源，表中内容如下图：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszdh2kj20dm0deaah.jpg" alt="mysql"></p><h2 id="2-2-读取数据"><a href="#2-2-读取数据" class="headerlink" title="2.2 读取数据"></a>2.2 读取数据</h2><p>Spark可以用JDBC来连接关系型数据库，包括MySQL、Oracle、Postgre等系统。<br>在执行<code>spark-shell</code>或者<code>spark-submit</code>命令的时候，需在<code>--driver-class-path</code>配置对应数据库的JDBC驱动的路径。<br>本例中，使用以下命令启动spark-shell：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --driver-class-path /home/hadoop/mysql-connector-java-5.1.21-bin.jar</span><br></pre></td></tr></table></figure><p></p><h3 id="2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD"><a href="#2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD" class="headerlink" title="2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD"></a>2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD</h3><p>代码及说明如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">DriverManager</span></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">ResultSet</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createConnection</span></span>() = &#123;              <span class="comment">//创建连接</span></span><br><span class="line">  <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line"><span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/db_score"</span>,<span class="string">"root"</span>,<span class="string">"passwd"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractValues</span></span>(r:<span class="type">ResultSet</span>) = &#123;      <span class="comment">//从数据库中取得数据后转换格式</span></span><br><span class="line">  (r.getInt(<span class="number">1</span>),r.getString(<span class="number">2</span>))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> courseRdd = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(            <span class="comment">// 调用JdbcRDD类</span></span><br><span class="line">  sc,                                   <span class="comment">// SparkContext对象</span></span><br><span class="line">  createConnection,                     <span class="comment">// 与数据库的连接</span></span><br><span class="line">  <span class="string">"select * from tb_course where ? &lt;= courseid and courseid &lt;= ?"</span>, <span class="comment">// SQL语句</span></span><br><span class="line">  <span class="number">1</span>,                                    <span class="comment">// 查询的下界</span></span><br><span class="line">  <span class="number">7</span>,                                    <span class="comment">// 查询的上界</span></span><br><span class="line">  <span class="number">2</span>,                                    <span class="comment">// partition的个数(即分为几部分查询)</span></span><br><span class="line">  extractValues                         <span class="comment">// 将数据转换成需要的格式</span></span><br><span class="line">)</span><br><span class="line">courseRdd.collect.foreach(println)           <span class="comment">// 打印输出</span></span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszfc11j208p03rt8l.jpg" alt="courseRDD"></p><blockquote><p>注：从上例中可以看出，使用JdbcRDD时，SQL查询语句必须有类似<code>ID &gt;= ? AND ID &lt;= ?</code>这样的where语句（经测试，直接去掉会报错），而且上界和下界的类型必须是Long，这样使得JdbcRDD的使用场景比较局限。不过参照JdbcRDD的源代码，用户可以修改源代码以写出符合自己需求的JdbcRDD。</p></blockquote><h3 id="2-2-2-方法二：使用Spark-SQL来返回一个DataFrame"><a href="#2-2-2-方法二：使用Spark-SQL来返回一个DataFrame" class="headerlink" title="2.2.2 方法二：使用Spark SQL来返回一个DataFrame"></a>2.2.2 方法二：使用Spark SQL来返回一个DataFrame</h3><p>代码及说明如下：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line"></span><br><span class="line">val sqlContext = new SQLContext(sc)               // 生成SQLContext对象</span><br><span class="line">val sql = <span class="string">"select * from tb_course"</span>               // SQL查询语句</span><br><span class="line"></span><br><span class="line">val courseDF = sqlContext.read.format(<span class="string">"jdbc"</span>).options(</span><br><span class="line">  Map(<span class="string">"url"</span>-&gt;<span class="string">"jdbc:mysql://localhost:3306/db_score"</span>,</span><br><span class="line">    <span class="string">"dbtable"</span>-&gt;s<span class="string">"(<span class="variable">$&#123;sql&#125;</span>) as table01"</span>,            // SQL查询并对结果起别名</span><br><span class="line">    <span class="string">"driver"</span>-&gt;<span class="string">"com.mysql.jdbc.Driver"</span>,            // 驱动</span><br><span class="line">    <span class="string">"user"</span>-&gt; <span class="string">"root"</span>,                              // 用户名</span><br><span class="line">    <span class="string">"password"</span>-&gt;<span class="string">"passwd"</span>)                         // 密码</span><br><span class="line">).load()</span><br><span class="line"></span><br><span class="line">courseDF.collect().foreach(println)               // 打印输出</span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszgge9j208h03pwed.jpg" alt="courseDF"></p><h1 id="3-从HBase数据库中读数据"><a href="#3-从HBase数据库中读数据" class="headerlink" title="3 从HBase数据库中读数据"></a>3 从HBase数据库中读数据</h1><h2 id="3-1-准备数据"><a href="#3-1-准备数据" class="headerlink" title="3.1 准备数据"></a>3.1 准备数据</h2><p>首先启动HDFS（<code>start-dfs.sh</code>）和HBase（<code>start-hbase.sh</code>）<br>输入<code>hbase shell</code>进入HBase的命令行模式<br>使用create命令创建一张有f1、f2两个列族的表：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main) &gt; create <span class="string">'test1'</span>,&#123;NAME =&gt; <span class="string">'f1'</span>&#125;,&#123;NAME =&gt; <span class="string">'f2'</span>&#125;</span><br></pre></td></tr></table></figure><p></p><p>使用put命令给表<code>test1</code>添加一些测试数据：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row01'</span>,<span class="string">'f1:data'</span>,<span class="string">'10001'</span> </span><br><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row01'</span>,<span class="string">'f2:data'</span>,<span class="string">'10002'</span></span><br><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row02'</span>,<span class="string">'f2:data'</span>,<span class="string">'10003'</span></span><br></pre></td></tr></table></figure><p></p><p>查看添加的数据：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszck52j20kh02lq2w.jpg" alt="scantest1"></p><h2 id="3-2-读取数据"><a href="#3-2-读取数据" class="headerlink" title="3.2 读取数据"></a>3.2 读取数据</h2><p>Spark连接HBase时需要一些必要的jar包，可在HBase安装目录下的lib文件夹中找到，将它们复制到一个自定义文件夹中（本例中在Spark安装目录下新建了名为hbase-lib的文件夹），这些jar包清单如下：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszd8wyj20ka03y0su.jpg" alt="sparkhbasejars"><br>即metrics-core-2.2.0.jar、protobuf-java-2.5.0.jar、htrace-core-3.1.0-incubating.jar、guava-12.0.1.jar这四个jar包加上所有hbase-开头的所有jar包。（注：spark的环境中有metrics的jar包，但是可能是版本不匹配的问题，如果不加入此2.2.0版本的，程序会报错）<br>然后在Spark安装目录下的conf文件夹中找到<code>spark-env.sh</code>,在其中添加：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_CLASSPATH=/opt/software/spark/hbase-lib/*</span><br></pre></td></tr></table></figure><p></p><h3 id="3-2-1-方法一：调用newAPIHadoopRDD"><a href="#3-2-1-方法一：调用newAPIHadoopRDD" class="headerlink" title="3.2.1 方法一：调用newAPIHadoopRDD"></a>3.2.1 方法一：调用<code>newAPIHadoopRDD</code></h3><p>代码及相关说明如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.<span class="type">Result</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>,<span class="string">"test1"</span>)   <span class="comment">//设置需要扫描的表(test1)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sc.newAPIHadoopRDD(conf,</span><br><span class="line">classOf[<span class="type">TableInputFormat</span>],classOf[<span class="type">ImmutableBytesWritable</span>],classOf[<span class="type">Result</span>])</span><br></pre></td></tr></table></figure><p></p><p>由于<code>TableInputFormat</code>类的实现，Spark可以用Hadoop输入格式访问HBase，即调用<code>sc.newAPIHadoopRDD</code>，此方法返回一个键值对类型的RDD，其中键的类型为<code>ImmutableBytesWritable</code>，值的类型为<code>Result</code>（分别是此方法的后两个参数）。<br>因此，遍历此键值对RDD中的值即可取得想要的数据，代码如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach&#123;<span class="keyword">case</span> (_,result) =&gt;&#123;              <span class="comment">//逐行遍历</span></span><br><span class="line">  <span class="keyword">val</span> row = <span class="type">Bytes</span>.toString(result.getRow)    <span class="comment">//获取当前行的Row key</span></span><br><span class="line">  <span class="keyword">val</span> value = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"f2"</span>.getBytes,<span class="string">"data"</span>.getBytes))</span><br><span class="line">                            <span class="comment">//根据列族名(f2)和列名(data)取当前行的数据</span></span><br><span class="line">  println(<span class="string">"Row:"</span>+row+<span class="string">" f2, data:"</span>+value)     <span class="comment">//打印输出</span></span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure><p></p><p>运行结果如下：<br><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fs1aszcqjlj20gw03fgli.jpg" alt=""></p><h3 id="3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法"><a href="#3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法" class="headerlink" title="3.2.2 方法二：用org.apache.hadoop.hbase中提供的方法"></a>3.2.2 方法二：用<code>org.apache.hadoop.hbase</code>中提供的方法</h3><p>以下代码改编自《Hadoop+Spark生态系统操作与实战指南》，利用此代码可以实现对HBase的CRUD操作，代码如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client._</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;<span class="type">HBaseConfiguration</span>, <span class="type">HColumnDescriptor</span>,</span><br><span class="line"><span class="type">HTableDescriptor</span>, <span class="type">TableName</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createHTable</span></span>(connection: <span class="type">Connection</span>,tablename: <span class="type">String</span>): <span class="type">Unit</span>=</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//Hbase表模式管理器</span></span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="comment">//本例将操作的表名</span></span><br><span class="line">  <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">  <span class="comment">//如果需要创建表</span></span><br><span class="line">  <span class="keyword">if</span> (!admin.tableExists(tableName)) &#123;</span><br><span class="line">    <span class="comment">//创建Hbase表模式</span></span><br><span class="line">    <span class="keyword">val</span> tableDescriptor = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(tableName)</span><br><span class="line">    <span class="comment">//创建列簇1    artitle</span></span><br><span class="line">    tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"artitle"</span>.getBytes()))</span><br><span class="line">    <span class="comment">//创建列簇2    author</span></span><br><span class="line">    tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"author"</span>.getBytes()))</span><br><span class="line">    <span class="comment">//创建表</span></span><br><span class="line">    admin.createTable(tableDescriptor)</span><br><span class="line">    println(<span class="string">"create done."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//删除表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteHTable</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="comment">//本例将操作的表名</span></span><br><span class="line">  <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">  <span class="comment">//Hbase表模式管理器</span></span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(tableName))&#123;</span><br><span class="line">    admin.disableTable(tableName)</span><br><span class="line">    admin.deleteTable(tableName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertHTable</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>,value:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    <span class="keyword">val</span> table=connection.getTable(userTable)</span><br><span class="line">    <span class="comment">//准备key 的数据</span></span><br><span class="line">    <span class="keyword">val</span> p=<span class="keyword">new</span> <span class="type">Put</span>(key.getBytes)</span><br><span class="line">    <span class="comment">//为put操作指定 column 和 value</span></span><br><span class="line">    p.addColumn(family.getBytes,column.getBytes,value.getBytes())</span><br><span class="line">    <span class="comment">//提交一行</span></span><br><span class="line">    table.put(p)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//基于KEY查询某条数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAResult</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> g=<span class="keyword">new</span> <span class="type">Get</span>(key.getBytes())</span><br><span class="line">    <span class="keyword">val</span> result=table.get(g)</span><br><span class="line">    <span class="keyword">val</span> value=<span class="type">Bytes</span>.toString(result.getValue(family.getBytes(),column.getBytes()))</span><br><span class="line">    println(<span class="string">"value:"</span>+value)</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)table.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除某条记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteRecord</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable=<span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> d=<span class="keyword">new</span> <span class="type">Delete</span>(key.getBytes())</span><br><span class="line">    d.addColumn(family.getBytes(),column.getBytes())</span><br><span class="line">    table.delete(d)</span><br><span class="line">    println(<span class="string">"delete record done."</span>)</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)table.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanRecord</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> scanner:<span class="type">ResultScanner</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable=<span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> s=<span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">    s.addColumn(family.getBytes(),column.getBytes())</span><br><span class="line">    scanner=table.getScanner(s)</span><br><span class="line">    println(<span class="string">"scan...for..."</span>)</span><br><span class="line">    <span class="keyword">var</span> result:<span class="type">Result</span>=scanner.next()</span><br><span class="line">    <span class="keyword">while</span>(result!=<span class="literal">null</span>)&#123;</span><br><span class="line">      println(<span class="string">"Found row:"</span> + result)</span><br><span class="line">      println(<span class="string">"Found value: "</span>+</span><br><span class="line"><span class="type">Bytes</span>.toString(result.getValue(family.getBytes(),column.getBytes())))</span><br><span class="line">      result=scanner.next()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)</span><br><span class="line">      table.close()</span><br><span class="line">    scanner.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>（注：以上代码中的Key均代表<code>Row Key</code>）<br>以上代码将在HBase中创建表、删除表、插入记录、根据行号查询数据、删除记录、扫描记录等操作都写成了函数，将以上代码在spark-shell中运行后，对HBase的操作直接调用相关函数即可，如下：<br></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个配置，采用的是工厂方法</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create</span><br><span class="line"><span class="comment">//Connection 的创建是个重量级的工作，线程安全，是操作hbase的入口</span></span><br><span class="line"><span class="keyword">val</span> connection= <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表测试</span></span><br><span class="line">createHTable(connection, <span class="string">"HadoopAndSpark"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入数据,重复执行为覆盖</span></span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"002"</span>,<span class="string">"Hadoop for me"</span>)</span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"003"</span>,<span class="string">"Java for me"</span>)</span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Spark"</span>,<span class="string">"002"</span>,<span class="string">"Scala for me"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除记录</span></span><br><span class="line">deleteRecord(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Spark"</span>,<span class="string">"002"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描整个表</span></span><br><span class="line">scanRecord(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据表名、行键、列族、列名取当前Cell的数据</span></span><br><span class="line">getAResult(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"002"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除表测试</span></span><br><span class="line">deleteHTable(connection, <span class="string">"HadoopAndSpark"</span>)</span><br></pre></td></tr></table></figure><p></p><h1 id="4-后记"><a href="#4-后记" class="headerlink" title="4 后记"></a>4 后记</h1><p>Spark可以通过所有Hadoop支持的外部数据源（包括本地文件系统、HDFS、Cassandra、关系型数据库、HBase、亚马逊S3等）建立RDD，本文没有讲到的，后续视情况补充。Spark支持文本文件、序列文件及其他任何Hadoop输入格式文件。</p><h1 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5 参考资料"></a>5 参考资料</h1><p>[1]Karau,H.&amp;A.Konwinski.Spark快速大数据分析[M].王道远译.北京:人民邮电出版社.2015-09:64-65,81-85<br>[2]余辉.Hadoop+Spark生态系统操作与实战指南[M].北京:清华大学出版社.2017:136-140</p></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/Spark/" rel="tag"># Spark</a> <a href="/tags/HDFS/" rel="tag"># HDFS</a> <a href="/tags/RDD/" rel="tag"># RDD</a> <a href="/tags/Hadoop/" rel="tag"># Hadoop</a> <a href="/tags/HBase/" rel="tag"># HBase</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Xv6学习小计1/" rel="next" title="Xv6学习小记（一）——编译与运行"><i class="fa fa-chevron-left"></i> Xv6学习小记（一）——编译与运行</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/Spark-RDD的简单使用/" rel="prev" title="Spark RDD的简单使用">Spark RDD的简单使用<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> Inhaltsverzeichnis</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> Übersicht</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Xu Qiming</p><p class="site-description motion-element" itemprop="description">软件工程硕士在读</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">22</span> <span class="site-state-item-name">Artikel</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">8</span> <span class="site-state-item-name">Kategorien</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">46</span> <span class="site-state-item-name">Tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-从HDFS中读数据"><span class="nav-number">1.</span> <span class="nav-text">1 从HDFS中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-准备数据"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 准备数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-读取数据"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 读取数据</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-从MySQL数据库中读数据"><span class="nav-number">2.</span> <span class="nav-text">2 从MySQL数据库中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-数据来源"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 数据来源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-读取数据"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 读取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-方法二：使用Spark-SQL来返回一个DataFrame"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 方法二：使用Spark SQL来返回一个DataFrame</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-从HBase数据库中读数据"><span class="nav-number">3.</span> <span class="nav-text">3 从HBase数据库中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-准备数据"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 准备数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-读取数据"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 读取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-方法一：调用newAPIHadoopRDD"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 方法一：调用newAPIHadoopRDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 方法二：用org.apache.hadoop.hbase中提供的方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-后记"><span class="nav-number">4.</span> <span class="nav-text">4 后记</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-参考资料"><span class="nav-number">5.</span> <span class="nav-text">5 参考资料</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span><span class="with-love" id="animate"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">Xu Qiming</span></div><div class="powered-by">Erstellt mit <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div> <span class="post-meta-divider">|</span><div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v6.5.0</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/js/src/utils.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.5.0"></script></body></html>