<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.8.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qiming.info","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","offset":12,"onmobile":true,"dimmer":false},"copycode":{"enable":true,"show_result":true},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"save":"auto"},"fancybox":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据">
<meta name="keywords" content="Spark,HDFS,RDD,Hadoop,HBase">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark从外部数据集中读取数据">
<meta property="og:url" content="https://qiming.info/Spark从外部数据集中读取数据/index.html">
<meta property="og:site_name" content="QIMING.INFO">
<meta property="og:description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://qiming.info/images/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg">
<meta property="og:image" content="https://qiming.info/images/b40cee9bly1fs1aszcryqj20g3040glj.jpg">
<meta property="og:image" content="https://qiming.info/images/b40cee9bly1fs1aszdh2kj20dm0deaah.jpg">
<meta property="og:image" content="https://qiming.info/images/b40cee9bly1fs1aszfc11j208p03rt8l.jpg">
<meta property="og:image" content="https://qiming.info/images/b40cee9bly1fs1aszgge9j208h03pwed.jpg">
<meta property="og:image" content="https://qiming.info/images/b40cee9bly1fs1aszck52j20kh02lq2w.jpg">
<meta property="og:image" content="https://qiming.info/images/b40cee9bly1fs1aszd8wyj20ka03y0su.jpg">
<meta property="og:image" content="https://qiming.info/images/b40cee9bly1fs1aszcqjlj20gw03fgli.jpg">
<meta property="og:updated_time" content="2021-04-20T16:20:50.637Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark从外部数据集中读取数据">
<meta name="twitter:description" content="本文将介绍几种从Spark中读取数据存入RDD的方式，分别是  从HDFS中读数据 从MySQL数据库中读数据 从HBase数据库中读数据">
<meta name="twitter:image" content="https://qiming.info/images/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg">

<link rel="canonical" href="https://qiming.info/Spark从外部数据集中读取数据/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "6629f244"
    });
  daovoice('update');
  </script>



   <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas> 
   <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
   <script type="text/javascript" src="/js/fireworks.js"></script>

  <title>Spark从外部数据集中读取数据 | QIMING.INFO</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta custom-logo">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">QIMING.INFO</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">You are more than what you have become now.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-关于-/-留言板">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于 / 留言板</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">48</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">8</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">23</span></a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://qiming.info/Spark从外部数据集中读取数据/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Xu Qiming">
      <meta itemprop="description" content="软件工程硕士在读">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QIMING.INFO">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark从外部数据集中读取数据
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-06-06 11:12:24" itemprop="dateCreated datePublished" datetime="2018-06-06T11:12:24+08:00">2018-06-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          
            <span id="/Spark从外部数据集中读取数据/" class="post-meta-item leancloud_visitors" data-flag-title="Spark从外部数据集中读取数据" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/Spark从外部数据集中读取数据/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Spark从外部数据集中读取数据/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文将介绍几种从Spark中读取数据存入RDD的方式，分别是</p>
<ul>
<li>从HDFS中读数据</li>
<li>从MySQL数据库中读数据</li>
<li>从HBase数据库中读数据</li>
</ul>
<a id="more"></a>
<p>本文中涉及到的工具版本如下：</p>
<ul>
<li>Hadoop：2.7.4</li>
<li>Spark：2.1.1</li>
<li>HBase：1.2.6</li>
<li>MySQL：5.7.22</li>
<li>JDK：1.8.0_171</li>
<li>Scala：2.11.8</li>
</ul>
<h1 id="1-从HDFS中读数据"><a href="#1-从HDFS中读数据" class="headerlink" title="1 从HDFS中读数据"></a>1 从HDFS中读数据</h1><h2 id="1-1-准备数据"><a href="#1-1-准备数据" class="headerlink" title="1.1 准备数据"></a>1.1 准备数据</h2><p>首先启动Hadoop（使用<code>start-dfs.sh</code>），在HDFS上创建一个目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -mkdir -p /user/hadoop/input</span><br></pre></td></tr></table></figure></p>
<p>新建一个文件<code>input.txt</code>，内容如下：<br><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">15 78 89 22</span><br><span class="line">777 32 4 50</span><br></pre></td></tr></table></figure></p>
<p>将<code>input.txt</code>上传到HDFS上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put input.txt /user/hadoop/input</span><br></pre></td></tr></table></figure></p>
<p>用ls命令查看是否上传成功：<br><img src="../images/b40cee9bly1fs1aszcqyej20ie01oa9y.jpg" alt=""></p>
<h2 id="1-2-读取数据"><a href="#1-2-读取数据" class="headerlink" title="1.2 读取数据"></a>1.2 读取数据</h2><p>Spark将读取到的数据会保存在RDD中，关于RDD的介绍可以参考本站的这篇文章<a href="https://qiming.info/Spark-RDD的简单使用">Spark-RDD的简单使用</a>。<br>在Spark中从HDFS读取文本文件可以使用<code>sc.textFile</code>方法，将此方法的参数设为<code>hdfs://master:port/path</code>即可。<br>所以本例中的读取步骤如下：<br>进入spark的安装目录，使用<code>bin/spark-shell</code>来启动<code>spark</code>命令行编程（语言为<code>scala</code>）。<br>输入以下代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"hdfs://localhost:9000/user/hadoop/input/input.txt"</span>)</span><br><span class="line">rdd.count()           <span class="comment">// 输出行数</span></span><br><span class="line">rdd.foreach(println)  <span class="comment">// 将所有内容打印出来</span></span><br></pre></td></tr></table></figure></p>
<p><img src="../images/b40cee9bly1fs1aszcryqj20g3040glj.jpg" alt=""></p>
<h1 id="2-从MySQL数据库中读数据"><a href="#2-从MySQL数据库中读数据" class="headerlink" title="2 从MySQL数据库中读数据"></a>2 从MySQL数据库中读数据</h1><h2 id="2-1-数据来源"><a href="#2-1-数据来源" class="headerlink" title="2.1 数据来源"></a>2.1 数据来源</h2><p>将db_score数据库中的tb_course表作为数据来源，表中内容如下图：<br><img src="../images/b40cee9bly1fs1aszdh2kj20dm0deaah.jpg" alt=""></p>
<h2 id="2-2-读取数据"><a href="#2-2-读取数据" class="headerlink" title="2.2 读取数据"></a>2.2 读取数据</h2><p>Spark可以用JDBC来连接关系型数据库，包括MySQL、Oracle、Postgre等系统。<br>在执行<code>spark-shell</code>或者<code>spark-submit</code>命令的时候，需在<code>--driver-class-path</code>配置对应数据库的JDBC驱动的路径。<br>本例中，使用以下命令启动spark-shell：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --driver-class-path /home/hadoop/mysql-connector-java-5.1.21-bin.jar</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD"><a href="#2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD" class="headerlink" title="2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD"></a>2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD</h3><p>代码及说明如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">DriverManager</span></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">ResultSet</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createConnection</span></span>() = &#123;              <span class="comment">//创建连接</span></span><br><span class="line">  <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line"><span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/db_score"</span>,<span class="string">"root"</span>,<span class="string">"passwd"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractValues</span></span>(r:<span class="type">ResultSet</span>) = &#123;      <span class="comment">//从数据库中取得数据后转换格式</span></span><br><span class="line">  (r.getInt(<span class="number">1</span>),r.getString(<span class="number">2</span>))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> courseRdd = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(            <span class="comment">// 调用JdbcRDD类</span></span><br><span class="line">  sc,                                   <span class="comment">// SparkContext对象</span></span><br><span class="line">  createConnection,                     <span class="comment">// 与数据库的连接</span></span><br><span class="line">  <span class="string">"select * from tb_course where ? &lt;= courseid and courseid &lt;= ?"</span>, <span class="comment">// SQL语句</span></span><br><span class="line">  <span class="number">1</span>,                                    <span class="comment">// 查询的下界</span></span><br><span class="line">  <span class="number">7</span>,                                    <span class="comment">// 查询的上界</span></span><br><span class="line">  <span class="number">2</span>,                                    <span class="comment">// partition的个数(即分为几部分查询)</span></span><br><span class="line">  extractValues                         <span class="comment">// 将数据转换成需要的格式</span></span><br><span class="line">)</span><br><span class="line">courseRdd.collect.foreach(println)           <span class="comment">// 打印输出</span></span><br></pre></td></tr></table></figure></p>
<p>结果如下图：<br><img src="../images/b40cee9bly1fs1aszfc11j208p03rt8l.jpg" alt=""></p>
<blockquote>
<p>注：从上例中可以看出，使用JdbcRDD时，SQL查询语句必须有类似<code>ID &gt;= ? AND ID &lt;= ?</code>这样的where语句（经测试，直接去掉会报错），而且上界和下界的类型必须是Long，这样使得JdbcRDD的使用场景比较局限。不过参照JdbcRDD的源代码，用户可以修改源代码以写出符合自己需求的JdbcRDD。</p>
</blockquote>
<h3 id="2-2-2-方法二：使用Spark-SQL来返回一个DataFrame"><a href="#2-2-2-方法二：使用Spark-SQL来返回一个DataFrame" class="headerlink" title="2.2.2 方法二：使用Spark SQL来返回一个DataFrame"></a>2.2.2 方法二：使用Spark SQL来返回一个DataFrame</h3><p>代码及说明如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line"></span><br><span class="line">val sqlContext = new SQLContext(sc)               // 生成SQLContext对象</span><br><span class="line">val sql = <span class="string">"select * from tb_course"</span>               // SQL查询语句</span><br><span class="line"></span><br><span class="line">val courseDF = sqlContext.read.format(<span class="string">"jdbc"</span>).options(</span><br><span class="line">  Map(<span class="string">"url"</span>-&gt;<span class="string">"jdbc:mysql://localhost:3306/db_score"</span>,</span><br><span class="line">    <span class="string">"dbtable"</span>-&gt;s<span class="string">"(<span class="variable">$&#123;sql&#125;</span>) as table01"</span>,            // SQL查询并对结果起别名</span><br><span class="line">    <span class="string">"driver"</span>-&gt;<span class="string">"com.mysql.jdbc.Driver"</span>,            // 驱动</span><br><span class="line">    <span class="string">"user"</span>-&gt; <span class="string">"root"</span>,                              // 用户名</span><br><span class="line">    <span class="string">"password"</span>-&gt;<span class="string">"passwd"</span>)                         // 密码</span><br><span class="line">).load()</span><br><span class="line"></span><br><span class="line">courseDF.collect().foreach(println)               // 打印输出</span><br></pre></td></tr></table></figure></p>
<p>结果如下图：<br><img src="../images/b40cee9bly1fs1aszgge9j208h03pwed.jpg" alt=""></p>
<h1 id="3-从HBase数据库中读数据"><a href="#3-从HBase数据库中读数据" class="headerlink" title="3 从HBase数据库中读数据"></a>3 从HBase数据库中读数据</h1><h2 id="3-1-准备数据"><a href="#3-1-准备数据" class="headerlink" title="3.1 准备数据"></a>3.1 准备数据</h2><p>首先启动HDFS（<code>start-dfs.sh</code>）和HBase（<code>start-hbase.sh</code>）<br>输入<code>hbase shell</code>进入HBase的命令行模式<br>使用create命令创建一张有f1、f2两个列族的表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main) &gt; create <span class="string">'test1'</span>,&#123;NAME =&gt; <span class="string">'f1'</span>&#125;,&#123;NAME =&gt; <span class="string">'f2'</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>使用put命令给表<code>test1</code>添加一些测试数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row01'</span>,<span class="string">'f1:data'</span>,<span class="string">'10001'</span> </span><br><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row01'</span>,<span class="string">'f2:data'</span>,<span class="string">'10002'</span></span><br><span class="line">hbase(main) &gt; put <span class="string">'test1'</span>,<span class="string">'row02'</span>,<span class="string">'f2:data'</span>,<span class="string">'10003'</span></span><br></pre></td></tr></table></figure></p>
<p>查看添加的数据：<br><img src="../images/b40cee9bly1fs1aszck52j20kh02lq2w.jpg" alt=""></p>
<h2 id="3-2-读取数据"><a href="#3-2-读取数据" class="headerlink" title="3.2 读取数据"></a>3.2 读取数据</h2><p>Spark连接HBase时需要一些必要的jar包，可在HBase安装目录下的lib文件夹中找到，将它们复制到一个自定义文件夹中（本例中在Spark安装目录下新建了名为hbase-lib的文件夹），这些jar包清单如下：<br><img src="../images/b40cee9bly1fs1aszd8wyj20ka03y0su.jpg" alt=""><br>即metrics-core-2.2.0.jar、protobuf-java-2.5.0.jar、htrace-core-3.1.0-incubating.jar、guava-12.0.1.jar这四个jar包加上所有hbase-开头的所有jar包。（注：spark的环境中有metrics的jar包，但是可能是版本不匹配的问题，如果不加入此2.2.0版本的，程序会报错）<br>然后在Spark安装目录下的conf文件夹中找到<code>spark-env.sh</code>,在其中添加：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_CLASSPATH=/opt/software/spark/hbase-lib/*</span><br></pre></td></tr></table></figure>
<h3 id="3-2-1-方法一：调用newAPIHadoopRDD"><a href="#3-2-1-方法一：调用newAPIHadoopRDD" class="headerlink" title="3.2.1 方法一：调用newAPIHadoopRDD"></a>3.2.1 方法一：调用<code>newAPIHadoopRDD</code></h3><p>代码及相关说明如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.<span class="type">Result</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>,<span class="string">"test1"</span>)   <span class="comment">//设置需要扫描的表(test1)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sc.newAPIHadoopRDD(conf,</span><br><span class="line">classOf[<span class="type">TableInputFormat</span>],classOf[<span class="type">ImmutableBytesWritable</span>],classOf[<span class="type">Result</span>])</span><br></pre></td></tr></table></figure></p>
<p>由于<code>TableInputFormat</code>类的实现，Spark可以用Hadoop输入格式访问HBase，即调用<code>sc.newAPIHadoopRDD</code>，此方法返回一个键值对类型的RDD，其中键的类型为<code>ImmutableBytesWritable</code>，值的类型为<code>Result</code>（分别是此方法的后两个参数）。<br>因此，遍历此键值对RDD中的值即可取得想要的数据，代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach&#123;<span class="keyword">case</span> (_,result) =&gt;&#123;              <span class="comment">//逐行遍历</span></span><br><span class="line">  <span class="keyword">val</span> row = <span class="type">Bytes</span>.toString(result.getRow)    <span class="comment">//获取当前行的Row key</span></span><br><span class="line">  <span class="keyword">val</span> value = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"f2"</span>.getBytes,<span class="string">"data"</span>.getBytes))</span><br><span class="line">                            <span class="comment">//根据列族名(f2)和列名(data)取当前行的数据</span></span><br><span class="line">  println(<span class="string">"Row:"</span>+row+<span class="string">" f2, data:"</span>+value)     <span class="comment">//打印输出</span></span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><img src="../images/b40cee9bly1fs1aszcqjlj20gw03fgli.jpg" alt=""></p>
<h3 id="3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法"><a href="#3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法" class="headerlink" title="3.2.2 方法二：用org.apache.hadoop.hbase中提供的方法"></a>3.2.2 方法二：用<code>org.apache.hadoop.hbase</code>中提供的方法</h3><p>以下代码改编自《Hadoop+Spark生态系统操作与实战指南》，利用此代码可以实现对HBase的CRUD操作，代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client._</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;<span class="type">HBaseConfiguration</span>, <span class="type">HColumnDescriptor</span>,</span><br><span class="line"><span class="type">HTableDescriptor</span>, <span class="type">TableName</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createHTable</span></span>(connection: <span class="type">Connection</span>,tablename: <span class="type">String</span>): <span class="type">Unit</span>=</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//Hbase表模式管理器</span></span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="comment">//本例将操作的表名</span></span><br><span class="line">  <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">  <span class="comment">//如果需要创建表</span></span><br><span class="line">  <span class="keyword">if</span> (!admin.tableExists(tableName)) &#123;</span><br><span class="line">    <span class="comment">//创建Hbase表模式</span></span><br><span class="line">    <span class="keyword">val</span> tableDescriptor = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(tableName)</span><br><span class="line">    <span class="comment">//创建列簇1    artitle</span></span><br><span class="line">    tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"artitle"</span>.getBytes()))</span><br><span class="line">    <span class="comment">//创建列簇2    author</span></span><br><span class="line">    tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"author"</span>.getBytes()))</span><br><span class="line">    <span class="comment">//创建表</span></span><br><span class="line">    admin.createTable(tableDescriptor)</span><br><span class="line">    println(<span class="string">"create done."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//删除表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteHTable</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="comment">//本例将操作的表名</span></span><br><span class="line">  <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">  <span class="comment">//Hbase表模式管理器</span></span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(tableName))&#123;</span><br><span class="line">    admin.disableTable(tableName)</span><br><span class="line">    admin.deleteTable(tableName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertHTable</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>,value:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    <span class="keyword">val</span> table=connection.getTable(userTable)</span><br><span class="line">    <span class="comment">//准备key 的数据</span></span><br><span class="line">    <span class="keyword">val</span> p=<span class="keyword">new</span> <span class="type">Put</span>(key.getBytes)</span><br><span class="line">    <span class="comment">//为put操作指定 column 和 value</span></span><br><span class="line">    p.addColumn(family.getBytes,column.getBytes,value.getBytes())</span><br><span class="line">    <span class="comment">//提交一行</span></span><br><span class="line">    table.put(p)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//基于KEY查询某条数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAResult</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>):<span class="type">Unit</span>=&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable = <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> g=<span class="keyword">new</span> <span class="type">Get</span>(key.getBytes())</span><br><span class="line">    <span class="keyword">val</span> result=table.get(g)</span><br><span class="line">    <span class="keyword">val</span> value=<span class="type">Bytes</span>.toString(result.getValue(family.getBytes(),column.getBytes()))</span><br><span class="line">    println(<span class="string">"value:"</span>+value)</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)table.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除某条记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteRecord</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>,</span><br><span class="line">key:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable=<span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> d=<span class="keyword">new</span> <span class="type">Delete</span>(key.getBytes())</span><br><span class="line">    d.addColumn(family.getBytes(),column.getBytes())</span><br><span class="line">    table.delete(d)</span><br><span class="line">    println(<span class="string">"delete record done."</span>)</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)table.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描记录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanRecord</span></span>(connection:<span class="type">Connection</span>,tablename:<span class="type">String</span>,family:<span class="type">String</span>,column:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">var</span> table:<span class="type">Table</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> scanner:<span class="type">ResultScanner</span>=<span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">val</span> userTable=<span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">    table=connection.getTable(userTable)</span><br><span class="line">    <span class="keyword">val</span> s=<span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">    s.addColumn(family.getBytes(),column.getBytes())</span><br><span class="line">    scanner=table.getScanner(s)</span><br><span class="line">    println(<span class="string">"scan...for..."</span>)</span><br><span class="line">    <span class="keyword">var</span> result:<span class="type">Result</span>=scanner.next()</span><br><span class="line">    <span class="keyword">while</span>(result!=<span class="literal">null</span>)&#123;</span><br><span class="line">      println(<span class="string">"Found row:"</span> + result)</span><br><span class="line">      println(<span class="string">"Found value: "</span>+</span><br><span class="line"><span class="type">Bytes</span>.toString(result.getValue(family.getBytes(),column.getBytes())))</span><br><span class="line">      result=scanner.next()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(table!=<span class="literal">null</span>)</span><br><span class="line">      table.close()</span><br><span class="line">    scanner.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>（注：以上代码中的Key均代表<code>Row Key</code>）<br>以上代码将在HBase中创建表、删除表、插入记录、根据行号查询数据、删除记录、扫描记录等操作都写成了函数，将以上代码在spark-shell中运行后，对HBase的操作直接调用相关函数即可，如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个配置，采用的是工厂方法</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create</span><br><span class="line"><span class="comment">//Connection 的创建是个重量级的工作，线程安全，是操作hbase的入口</span></span><br><span class="line"><span class="keyword">val</span> connection= <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建表测试</span></span><br><span class="line">createHTable(connection, <span class="string">"HadoopAndSpark"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入数据,重复执行为覆盖</span></span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"002"</span>,<span class="string">"Hadoop for me"</span>)</span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"003"</span>,<span class="string">"Java for me"</span>)</span><br><span class="line">insertHTable(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Spark"</span>,<span class="string">"002"</span>,<span class="string">"Scala for me"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除记录</span></span><br><span class="line">deleteRecord(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Spark"</span>,<span class="string">"002"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描整个表</span></span><br><span class="line">scanRecord(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据表名、行键、列族、列名取当前Cell的数据</span></span><br><span class="line">getAResult(connection,<span class="string">"HadoopAndSpark"</span>,<span class="string">"artitle"</span>,<span class="string">"Hadoop"</span>,<span class="string">"002"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除表测试</span></span><br><span class="line">deleteHTable(connection, <span class="string">"HadoopAndSpark"</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="4-后记"><a href="#4-后记" class="headerlink" title="4 后记"></a>4 后记</h1><p>Spark可以通过所有Hadoop支持的外部数据源（包括本地文件系统、HDFS、Cassandra、关系型数据库、HBase、亚马逊S3等）建立RDD，本文没有讲到的，后续视情况补充。Spark支持文本文件、序列文件及其他任何Hadoop输入格式文件。</p>
<h1 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5 参考资料"></a>5 参考资料</h1><p>[1]Karau,H.&amp;A.Konwinski.Spark快速大数据分析[M].王道远译.北京:人民邮电出版社.2015-09:64-65,81-85<br>[2]余辉.Hadoop+Spark生态系统操作与实战指南[M].北京:清华大学出版社.2017:136-140</p>

    </div>

    
    
    
        <div class="reward-container">
  <div><div>-----本文结束<i class="fa fa-smile-o"></i>感谢您的阅读-----<hr>如我有幸帮到了您，那么，不妨<i class="far fa-hand-point-down"></i><i class="far fa-hand-point-down"></i><i class="far fa-hand-point-down"></i>~~~谢谢！</div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Xu Qiming 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Xu Qiming 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Xu Qiming
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://qiming.info/Spark从外部数据集中读取数据/" title="Spark从外部数据集中读取数据">https://qiming.info/Spark从外部数据集中读取数据/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"># Spark</a>
              <a href="/tags/HDFS/" rel="tag"># HDFS</a>
              <a href="/tags/RDD/" rel="tag"># RDD</a>
              <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
              <a href="/tags/HBase/" rel="tag"># HBase</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Xv6学习小计1/" rel="prev" title="Xv6学习小记（一）——编译与运行">
      <i class="fa fa-chevron-left"></i> Xv6学习小记（一）——编译与运行
    </a></div>
      <div class="post-nav-item">
    <a href="/Spark-RDD的简单使用/" rel="next" title="Spark RDD的简单使用">
      Spark RDD的简单使用 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-从HDFS中读数据"><span class="nav-text">1 从HDFS中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-准备数据"><span class="nav-text">1.1 准备数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-读取数据"><span class="nav-text">1.2 读取数据</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-从MySQL数据库中读数据"><span class="nav-text">2 从MySQL数据库中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-数据来源"><span class="nav-text">2.1 数据来源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-读取数据"><span class="nav-text">2.2 读取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-方法一：使用org-apache-spark-rdd-JdbcRDD"><span class="nav-text">2.2.1 方法一：使用org.apache.spark.rdd.JdbcRDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-方法二：使用Spark-SQL来返回一个DataFrame"><span class="nav-text">2.2.2 方法二：使用Spark SQL来返回一个DataFrame</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-从HBase数据库中读数据"><span class="nav-text">3 从HBase数据库中读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-准备数据"><span class="nav-text">3.1 准备数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-读取数据"><span class="nav-text">3.2 读取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-方法一：调用newAPIHadoopRDD"><span class="nav-text">3.2.1 方法一：调用newAPIHadoopRDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-方法二：用org-apache-hadoop-hbase中提供的方法"><span class="nav-text">3.2.2 方法二：用org.apache.hadoop.hbase中提供的方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-后记"><span class="nav-text">4 后记</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-参考资料"><span class="nav-text">5 参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xu Qiming" src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Xu Qiming</p>
  <div class="site-description" itemprop="description">软件工程硕士在读</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Scorpio-xu" title="GitHub → https://github.com/Scorpio-xu" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/xqming" title="知乎 → https://www.zhihu.com/people/xqming" rel="noopener" target="_blank"><i class="quora fa-fw"></i>知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/3020746395" title="微博 → https://weibo.com/u/3020746395" rel="noopener" target="_blank"><i class="weibo fa-fw"></i>微博</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xqmcode@gmail.com" title="E-Mail → mailto:xqmcode@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://zhangrenyu.com/" title="https://zhangrenyu.com/" rel="noopener" target="_blank">张仁宇</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yingchi.io/" title="https://yingchi.io/" rel="noopener" target="_blank">YINGCHI_Joey</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">陕ICP备18006087号 </a>
  </div>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xu Qiming</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  <script src="/js/local-search.js"></script>












  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : true,
      notify     : true,
      appId      : 'YFNNFJwKUBIii7YuKvisDgx5-gzGzoHsz',
      appKey     : 'dQtRYG8ToS3H5xs4c0L2sQtW',
      placeholder: "评论前输入您的邮箱能收到回复提醒哦~",
      avatar     : 'retro',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : ,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
