<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="description" content="TensorFlow快速上手"><meta name="keywords" content="神经网络,TensorFlow,Python,回归"><meta name="author" content="Xu Qiming,undefined"><meta name="copyright" content="Xu Qiming"><title>TensorFlow快速上手 | QIMING.INFO</title><link rel="shortcut icon" href="/my-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容:${query}"}},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"}}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-安装"><span class="toc-text">1 安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-使用"><span class="toc-text">2 使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-tensor"><span class="toc-text">2.1 tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-常量"><span class="toc-text">2.1.1 常量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-变量"><span class="toc-text">2.1.2 变量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Session"><span class="toc-text">2.2 Session</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Graph"><span class="toc-text">2.3 Graph</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-拟合线性回归"><span class="toc-text">2.4 拟合线性回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-参考资料"><span class="toc-text">3 参考资料</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://ws1.sinaimg.cn/large/b40cee9bly1fx6sa0kwotj205k05nglk.jpg"></div><div class="author-info__name text-center">Xu Qiming</div><div class="author-info__description text-center">You are more than what you have become now.</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">22</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">46</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">8</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container" style="background-image:url(true)"><div id="page-header"> <span class="pull-left"><a id="site-name" href="/">QIMING.INFO</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i> <span>搜索</span></a><a class="site-page" href="/">首页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/about">关于 / 留言板</a></span></div><div id="post-info"><div id="post-title">TensorFlow快速上手</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-05</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/深度学习/">深度学习</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/TensorFlow快速上手/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="TensorFlow快速上手/"></span></a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>TensorFlow是目前很火的一款深度学习框架，其源码是用C++写的，保证了运行速度，其又提供了Python的接口，大大降低了程序猿们学习新语言的成本，所以在深度学习领域广为流行。</p><p>但是很多人在初学TensorFlow时会觉得有些难以入手，霎时间接触诸如张量、图、会话等概念会有点吃力，所以本文将介绍如何快速入门TensorFlow并上手写代码，一边实践一边理解概念，提升学习速度。</p><a id="more"></a><h1 id="1-安装"><a href="#1-安装" class="headerlink" title="1 安装"></a>1 安装</h1><p>安装TensorFlow有多种方式，为了快速上手写代码，这里介绍一种最为简单的方法，像安装其他Python库一样，直接用pip就好，即在命令行中输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow</span><br></pre></td></tr></table></figure><blockquote><p>注1：还可以通过Docker安装或从源码安装。</p></blockquote><blockquote><p>注2：此处安装的是仅支持CPU版本的，支持GPU版本的将在后续文章中说明。</p></blockquote><h1 id="2-使用"><a href="#2-使用" class="headerlink" title="2 使用"></a>2 使用</h1><p>在使用的第一步，惯例我们先引入tensorflow库，为方便起见，将其用<code>tf</code>简写，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h2 id="2-1-tensor"><a href="#2-1-tensor" class="headerlink" title="2.1 tensor"></a>2.1 tensor</h2><p>tensor即张量，是TensorFlow中所有数据的基本表现形式，TensorFlow中的常量、变量都属于张量。</p><h3 id="2-1-1-常量"><a href="#2-1-1-常量" class="headerlink" title="2.1.1 常量"></a>2.1.1 常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.</span>,<span class="number">2.</span>],[<span class="number">3.</span>,<span class="number">4.</span>]],name=<span class="string">'const_a'</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><p>这段代码创建了一个常量a，直接输出后的结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"const_a:0"</span>, shape=(<span class="number">2</span>,<span class="number">2</span>), dtype=float32)</span><br></pre></td></tr></table></figure><p>可以看到直接输出的结果并不是我们给定的值，而是一个张量的结构。（输出值需要在会话中进行，下文会进行介绍）</p><p>其中<code>const_a:0</code>表示<code>a</code>是<code>const_a</code>的第一个值（也是唯一一个），shape代表维度，（2，2）表示此张量是个2x2的矩阵，第三个属性为数据类型。</p><p>常用的常数生成函数还有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 产生全是0的2x3矩阵</span></span><br><span class="line">tf.zeros([<span class="number">2</span>,<span class="number">3</span>],int32)</span><br><span class="line"><span class="comment"># 产生全是1的2x3矩阵</span></span><br><span class="line">tf.ones([<span class="number">2</span>,<span class="number">3</span>],int32)</span><br><span class="line"><span class="comment"># 产生全是9的2x3矩阵</span></span><br><span class="line">tf.fill([<span class="number">2</span>,<span class="number">3</span>],<span class="number">9</span>)</span><br></pre></td></tr></table></figure><h3 id="2-1-2-变量"><a href="#2-1-2-变量" class="headerlink" title="2.1.2 变量"></a>2.1.2 变量</h3><p>变量在TensorFlow中十分重要，因其可以在计算中修改，所以神经网络的模型参数一般使用变量。</p><p>创建变量可以直接调用Variable函数，将值传入即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个全1的2x2矩阵</span></span><br><span class="line">x = tf.Variable(tf.ones([<span class="number">2</span>,<span class="number">2</span>]),name=<span class="string">'var_x'</span>)</span><br></pre></td></tr></table></figure><p>需要注意的是，使用变量时，需要对其进行初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h2 id="2-2-Session"><a href="#2-2-Session" class="headerlink" title="2.2 Session"></a>2.2 Session</h2><p>Session即会话，TensorFlow中，所有操作都只能在会话中进行。</p><p>假如我们想求上文中创建的常量a和变量x的和，完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个2x2的矩阵常量a,值为[[1.0,2.0],[3.0,4.0]]</span></span><br><span class="line">a = tf.constant([[<span class="number">1.</span>,<span class="number">2.</span>],[<span class="number">3.</span>,<span class="number">4.</span>]],name=<span class="string">'const_a'</span>)</span><br><span class="line"><span class="comment"># 创建一个全1的2x2矩阵</span></span><br><span class="line">x = tf.Variable(tf.ones([<span class="number">2</span>,<span class="number">2</span>]),name=<span class="string">'var_x'</span>)</span><br><span class="line"><span class="comment"># 定义一个加法操作</span></span><br><span class="line">add = tf.add(a,x) <span class="comment"># 或 add = a + x</span></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 利用Python上下文管理器创建Session，并在其中执行有关操作</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 执行初始化变量</span></span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># 执行加法运算并输出结果</span></span><br><span class="line">    print(sess.run(add))</span><br></pre></td></tr></table></figure><blockquote><p>注1：创建常量、变量时的name参数可省略</p></blockquote><blockquote><p>注2：创建Session的另一种方法：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 调用会话来执行节点的计算，假如是上例中的add</span></span><br><span class="line">sess.run(add)</span><br><span class="line"><span class="comment"># 执行结束后关闭会话</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><p></p></blockquote><blockquote><p>这种方式不好的地方在于当程序因为异常而退出时，Session.close()可能不会执行从而导致资源泄露。</p></blockquote><p>运行后输出如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">2.</span> <span class="number">3.</span>]</span><br><span class="line"> [<span class="number">4.</span> <span class="number">5.</span>]]</span><br></pre></td></tr></table></figure><p></p><h2 id="2-3-Graph"><a href="#2-3-Graph" class="headerlink" title="2.3 Graph"></a>2.3 Graph</h2><p>至此，你是否隐约感受到了TensorFlow在运行时的一点不同？</p><p>不同于一般的程序，TensorFlow程序有两个阶段：</p><ol><li>定义计算</li><li>执行计算</li></ol><p>也就是说TensorFlow会将所有运算都定义好后再在Session中执行，不像一般程序，可以一边定义一边执行。</p><p>事实上，TensorFlow在进行第一个阶段时就是将计算定义在了一个图（Graph）里，在上文中，是将操作放到了TensorFlow默认提供的一张计算图中了，当然，我们也可以自己新建一张图，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一张图g</span></span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="comment"># 在图中定义常量c为5.0</span></span><br><span class="line">    c = tf.constant(<span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个Session，将图g作为参数传给它</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 输出c</span></span><br><span class="line">    print(sess.run(c))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 执行后，输出结果为：5.0</span></span><br></pre></td></tr></table></figure><h2 id="2-4-拟合线性回归"><a href="#2-4-拟合线性回归" class="headerlink" title="2.4 拟合线性回归"></a>2.4 拟合线性回归</h2><p>了解了上述基本概念和操作，接下来，我们来动手实践来拟合<code>y=0.1x+0.2</code>这个函数吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用numpy生成100个随机点</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>)</span><br><span class="line">y_data = x_data*<span class="number">0.1</span> + <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 给k随机赋初值2.2</span></span><br><span class="line">k = tf.Variable(<span class="number">2.2</span>)</span><br><span class="line"><span class="comment"># 给b随机赋初值1.1</span></span><br><span class="line">b = tf.Variable(<span class="number">1.1</span>)</span><br><span class="line"><span class="comment"># 构造一个线性模型</span></span><br><span class="line">y = k*x_data + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二次代价函数的损失值</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y_data-y))</span><br><span class="line"><span class="comment"># 定义一个梯度下降法来进行训练的优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 最小化代价函数</span></span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># 训练201次</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">201</span>):</span><br><span class="line">        sess.run(train)</span><br><span class="line">        <span class="comment"># 每训练20次 输出一下结果</span></span><br><span class="line">        <span class="keyword">if</span> step%<span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            print(step,sess.run([k,b]))</span><br></pre></td></tr></table></figure><p>训练结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> [<span class="number">1.7758392</span>, <span class="number">0.34141564</span>]</span><br><span class="line"><span class="number">20</span> [<span class="number">0.87541753</span>, <span class="number">-0.19211102</span>]</span><br><span class="line"><span class="number">40</span> [<span class="number">0.57061648</span>, <span class="number">-0.03798072</span>]</span><br><span class="line"><span class="number">60</span> [<span class="number">0.38562673</span>, <span class="number">0.055564649</span>]</span><br><span class="line"><span class="number">80</span> [<span class="number">0.27335268</span>, <span class="number">0.11233925</span>]</span><br><span class="line"><span class="number">100</span> [<span class="number">0.20521127</span>, <span class="number">0.14679691</span>]</span><br><span class="line"><span class="number">120</span> [<span class="number">0.16385485</span>, <span class="number">0.16770996</span>]</span><br><span class="line"><span class="number">140</span> [<span class="number">0.13875481</span>, <span class="number">0.18040252</span>]</span><br><span class="line"><span class="number">160</span> [<span class="number">0.12352109</span>, <span class="number">0.18810588</span>]</span><br><span class="line"><span class="number">180</span> [<span class="number">0.11427544</span>, <span class="number">0.19278121</span>]</span><br><span class="line"><span class="number">200</span> [<span class="number">0.10866406</span>, <span class="number">0.19561876</span>]</span><br></pre></td></tr></table></figure><p>可以看出，在训练了201次后，k值和b值都分别非常接近0.1和0.2了。</p><h1 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3 参考资料"></a>3 参考资料</h1><p>[1]TensorFlow中文社区.<a href="http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html" target="_blank" rel="noopener">基本用法 | TensorFlow 官方文档中文版</a><br>[2]郑泽宇,梁博文,顾思宇.TensorFlow:实战Goole深度学习框架(第2版)[M].北京:电子工业出版社.2018-02<br>[3]@Bilibili.<a href="https://www.bilibili.com/video/av20542427" target="_blank" rel="noopener">深度学习框架Tensorflow学习与应用</a>.2018-03</p></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined">Xu Qiming</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://qiming.info/TensorFlow快速上手/">https://qiming.info/TensorFlow快速上手/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://qiming.info" target="_blank">QIMING.INFO</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/神经网络/">神经网络</a><a class="post-meta__tags" href="/tags/TensorFlow/">TensorFlow</a><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/回归/">回归</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/TensorFlow实现简单神经网络/"><i class="fa fa-chevron-left"></i> <span>TensorFlow实现简单神经网络</span></a></div><div class="next-post pull-right"><a href="/ThoughtWorks校招作业之小型文本预处理器/"><span>ThoughtWorks校招作业之小型文本预处理器</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused=null,disqus_config=function(){this.page.url="https://qiming.info/TensorFlow快速上手/",this.page.identifier="TensorFlow快速上手/",this.page.title="TensorFlow快速上手"},d=document,s=d.createElement("script");s.src="https://.disqus.com/embed.js",s.setAttribute("data-timestamp",""+ +new Date),(d.head||d.body).appendChild(s)</script><script id="dsq-count-src" src="https://null.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By Xu Qiming</div><div class="framework-info"><span>驱动 -</span> <a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 -</span> <a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://qiming.info">blog</a>!</div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49b1f5">hexo-generator-search</a> <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>